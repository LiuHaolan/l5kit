{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swFeGE9Hu-eF"
   },
   "source": [
    "# Visualisation Examples\n",
    "\n",
    "This notebook shows some of the visualisation utility of our toolkit.\n",
    "\n",
    "The core packages for visualisation are:\n",
    "### `rasterization`\n",
    "contains classes for getting visual data as multi-channel tensors and turning them into interpretable RGB images.\n",
    "Every class has at least a `rasterize` method to get the tensor and a `to_rgb` method to convert it into an image.\n",
    "A few examples are:\n",
    "- `BoxRasterizer`: this object renders agents (e.g. vehicles or pedestrians) as oriented 2D boxes\n",
    "- `SatelliteRasterizer`: this object renders an oriented crop from a satellite map\n",
    "\n",
    "### `visualization`\n",
    "contains utilities to draw additional information (e.g. trajectories) onto RGB images. These utilities are commonly used after a `to_rgb` call to add other information to the final visualisation. \n",
    "One example is:\n",
    "- `draw_trajectory`: this function draws 2D trajectories from coordinates and yaws offset on an image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "-2bOYHKU1tw2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab.\n"
     ]
    }
   ],
   "source": [
    "#@title Download L5 Sample Dataset and install L5Kit\n",
    "import os\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !wget https://raw.githubusercontent.com/lyft/l5kit/master/examples/setup_notebook_colab.sh -q\n",
    "    !sh ./setup_notebook_colab.sh\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"] = open(\"./dataset_dir.txt\", \"r\").read().strip()\n",
    "else:\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"] = \"/mnt/scratch/v_liuhaolan/l5kit_data\"\n",
    "    print(\"Not running in Google Colab.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Xkf2xhE7u-eJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import os\n",
    "\n",
    "from l5kit.visualization.visualizer.zarr_utils import zarr_to_visualizer_scene\n",
    "from l5kit.visualization.visualizer.visualizer import visualize\n",
    "from bokeh.io import output_notebook, show\n",
    "from l5kit.data import MapAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS6kZYksu-eK"
   },
   "source": [
    "### First, let's configure where our data lives!\n",
    "The data is expected to live in a folder that can be configured using the `L5KIT_DATA_FOLDER` env variable. You data folder is expected to contain subfolders for the aerial and semantic maps as well as the scenes (`.zarr` files). \n",
    "In this example, the env variable is set to the local data folder. You should make sure the path points to the correct location for you.\n",
    "\n",
    "We built our code to work with a human-readable `yaml` config. This config file holds much useful information, however, we will only focus on a few functionalities concerning loading and visualization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zZwXO7Ybu-eK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'model_params': {'model_architecture': 'resnet50', 'history_num_frames': 4, 'future_num_frames': 12, 'step_time': 0.1, 'render_ego_history': True}, 'raster_params': {'raster_size': [160, 160], 'pixel_size': [0.5, 0.5], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'val_data_loader': {'key': 'scenes/train_full.zarr', 'batch_size': 64, 'shuffle': True, 'num_workers': 16}}\n"
     ]
    }
   ],
   "source": [
    "# Dataset is assumed to be on the folder specified\n",
    "# in the L5KIT_DATA_FOLDER environment variable\n",
    "\n",
    "# get config\n",
    "cfg = load_config_data(\"./visualization_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfMKTjgEu-eK"
   },
   "source": [
    "### We can look into our current configuration for interesting fields\n",
    "\n",
    "\\- when loaded in python, the `yaml`file is converted into a python `dict`. \n",
    "\n",
    "`raster_params` contains all the information related to the transformation of the 3D world onto an image plane:\n",
    "  - `raster_size`: the image plane size\n",
    "  - `pixel_size`: how many meters correspond to a pixel\n",
    "  - `ego_center`: our raster is centered around an agent, we can move the agent in the image plane with this param\n",
    "  - `map_type`: the rasterizer to be employed. We currently support a satellite-based and a semantic-based one. We will look at the differences further down in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GTu9RRD6u-eL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current raster_param:\n",
      "\n",
      "raster_size:[160, 160]\n",
      "pixel_size:[0.5, 0.5]\n",
      "ego_center:[0.25, 0.5]\n",
      "map_type:py_semantic\n",
      "satellite_map_key:aerial_map/aerial_map.png\n",
      "semantic_map_key:semantic_map/semantic_map.pb\n",
      "dataset_meta_key:meta.json\n",
      "filter_agents_threshold:0.5\n",
      "disable_traffic_light_faces:False\n",
      "set_origin_to_bottom:True\n"
     ]
    }
   ],
   "source": [
    "print(f'current raster_param:\\n')\n",
    "for k,v in cfg[\"raster_params\"].items():\n",
    "    print(f\"{k}:{v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmvxVhQRu-eL"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "The same config file is also used to load the data. Every split in the data has its own section, and multiple datasets can be used (as a whole or sliced). In this short example we will only use the first dataset from the `sample` set. You can change this by configuring the 'train_data_loader' variable in the config.\n",
    "\n",
    "You may also have noticed that we're building a `LocalDataManager` object. This will resolve relative paths from the config using the `L5KIT_DATA_FOLDER` env variable we have just set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fSuf-o1fu-eL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   134622   |  33437057  | 2655096629 |   314473872   |      928.68     |        248.38        |        79.41         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "dm = LocalDataManager()\n",
    "dataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\n",
    "zarr_dataset = ChunkedDataset(dataset_path)\n",
    "zarr_dataset.open()\n",
    "print(zarr_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkYsmiX8u-eL"
   },
   "source": [
    "## Working with the raw data\n",
    "\n",
    "`.zarr` files support most of the traditional numpy array operations. In the following cell we iterate over the frames to get a scatter plot of the AV locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zx6X7Hb4u-eL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nframes = zarr_dataset.frames\\ncoords = np.zeros((len(frames), 2))\\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\\n    frame = zarr_dataset.frames[idx_data]\\n    coords[idx_coord] = frame[\"ego_translation\"][:2]\\n\\n\\nplt.scatter(coords[:, 0], coords[:, 1], marker=\\'.\\')\\naxes = plt.gca()\\naxes.set_xlim([-2500, 1600])\\naxes.set_ylim([-2500, 1600])\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "frames = zarr_dataset.frames\n",
    "coords = np.zeros((len(frames), 2))\n",
    "for idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n",
    "    frame = zarr_dataset.frames[idx_data]\n",
    "    coords[idx_coord] = frame[\"ego_translation\"][:2]\n",
    "\n",
    "\n",
    "plt.scatter(coords[:, 0], coords[:, 1], marker='.')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-2500, 1600])\n",
    "axes.set_ylim([-2500, 1600])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG5NNUtNu-eM"
   },
   "source": [
    "Another easy thing to try is to get an idea of the agents types distribution. \n",
    "\n",
    "We can get all the agents `label_probabilities` and get the argmax for each raw. because `.zarr` files map to numpy array we can use all the traditional numpy operations and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "s31SlObxu-eM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nagents = zarr_dataset.agents\\nprobabilities = agents[\"label_probabilities\"]\\nlabels_indexes = np.argmax(probabilities, axis=1)\\ncounts = []\\nfor idx_label, label in enumerate(PERCEPTION_LABELS):\\n    counts.append(np.sum(labels_indexes == idx_label))\\n    \\ntable = PrettyTable(field_names=[\"label\", \"counts\"])\\nfor count, label in zip(counts, PERCEPTION_LABELS):\\n    table.add_row([label, count])\\nprint(table)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "agents = zarr_dataset.agents\n",
    "probabilities = agents[\"label_probabilities\"]\n",
    "labels_indexes = np.argmax(probabilities, axis=1)\n",
    "counts = []\n",
    "for idx_label, label in enumerate(PERCEPTION_LABELS):\n",
    "    counts.append(np.sum(labels_indexes == idx_label))\n",
    "    \n",
    "table = PrettyTable(field_names=[\"label\", \"counts\"])\n",
    "for count, label in zip(counts, PERCEPTION_LABELS):\n",
    "    table.add_row([label, count])\n",
    "print(table)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ug64ia6u-eM"
   },
   "source": [
    "## Working with data abstraction\n",
    "\n",
    "Even though it's absolutely fine to work with the raw data, we also provide classes that abstract data access to offer an easier way to generate inputs and targets.\n",
    "\n",
    "### Core Objects\n",
    "Along with the `rasterizer`, our toolkit contains other classes you may want to use while you build your solution. The `dataset` package, for example, already implements `PyTorch` ready datasets, so you can hit the ground running and start coding immediately.\n",
    "\n",
    "### Dataset package\n",
    "We will use two classes from the `dataset` package for this example. Both of them can be iterated and return multi-channel images from the rasterizer along with future trajectories offsets and other information.\n",
    "- `EgoDataset`: this dataset iterates over the AV annotations\n",
    "- `AgentDataset`: this dataset iterates over other agents annotations\n",
    "\n",
    "Both support multi-threading (through PyTorch DataLoader) OOB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "EeyavHW5u-eM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport cv2\\n\\nfor idx in range(30000,60000,8000):\\n    # plot same example with and without perturbation\\n    for perturbation_value in [1, 0]:\\n        perturbation.perturb_prob = perturbation_value\\n\\n        data_ego = val_dataset[idx]\\n        im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\\n        target_positions = transform_points(data_ego[\"target_positions\"], data_ego[\"raster_from_agent\"])\\n        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\\n        \\n#        print(data_ego[\"history_velocity\"])\\n        # is velocity also calibrated?\\n        print(data_ego[\"yaw\"])\\n        xy_points = data_ego[\"goal_pixel\"]\\n        cv2.circle(im_ego, (int(xy_points[0]),int(xy_points[1])), radius=2, color=(255, 0, 0), thickness=-1)\\n\\n    \\n        plt.imshow(im_ego)\\n        plt.axis(\\'off\\')\\n        plt.show()\\n\\n# before leaving, ensure perturb_prob is correct\\nperturbation.perturb_prob = perturb_prob\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from l5kit.kinematic import AckermanPerturbation\n",
    "from l5kit.random import GaussianRandomGenerator\n",
    "\n",
    "perturb_prob = 0.5\n",
    "\n",
    "# rasterisation and perturbation\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "#mean = np.array([0.0, 0.0, 0.0])  # lateral, longitudinal and angular\n",
    "#std = np.array([0.5, 1.5, np.pi / 6])\n",
    "\n",
    "mean = np.array([0.0, 0.0, 0.0])  # lateral, longitudinal and angular\n",
    "std = np.array([0.5, 1.5, np.pi / 6])\n",
    "\n",
    "perturbation = AckermanPerturbation(\n",
    "        random_offset_generator=GaussianRandomGenerator(mean=mean, std=std), perturb_prob=perturb_prob)\n",
    "\n",
    "# ===== INIT DATASET\n",
    "val_zarr = ChunkedDataset(dm.require(cfg[\"val_data_loader\"][\"key\"])).open()\n",
    "val_dataset = EgoDataset(cfg, val_zarr, rasterizer, perturbation)\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "\n",
    "for idx in range(30000,60000,8000):\n",
    "    # plot same example with and without perturbation\n",
    "    for perturbation_value in [1, 0]:\n",
    "        perturbation.perturb_prob = perturbation_value\n",
    "\n",
    "        data_ego = val_dataset[idx]\n",
    "        im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "        target_positions = transform_points(data_ego[\"target_positions\"], data_ego[\"raster_from_agent\"])\n",
    "        draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "        \n",
    "#        print(data_ego[\"history_velocity\"])\n",
    "        # is velocity also calibrated?\n",
    "        print(data_ego[\"yaw\"])\n",
    "        xy_points = data_ego[\"goal_pixel\"]\n",
    "        cv2.circle(im_ego, (int(xy_points[0]),int(xy_points[1])), radius=2, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "    \n",
    "        plt.imshow(im_ego)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# before leaving, ensure perturb_prob is correct\n",
    "perturbation.perturb_prob = perturb_prob\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2lFtn-zu-eN"
   },
   "source": [
    "## What if I want to visualise the Autonomous Vehicle (AV)?\n",
    "\n",
    "Let's get a sample from the dataset and use our `rasterizer` to get an RGB image we can plot. \n",
    "\n",
    "If we want to plot the ground truth trajectory, we can convert the dataset's `target_position` (displacements in meters in agent coordinates) into pixel coordinates in the image space, and call our utility function `draw_trajectory` (note that you can use this function for the predicted trajectories, as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "\n",
    "#print(len(dataset))\n",
    "\n",
    "data = dataset[14500]\n",
    "\n",
    "#im = data[\"image\"].transpose(1, 2, 0)\n",
    "im = data[\"image\"].transpose(1,2,0)\n",
    "im = dataset.rasterizer.to_rgb(im)\n",
    "\n",
    "#target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "#draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\n",
    "# how to solve the \"raster from agent\" prediction?\n",
    "\n",
    "#history_positions_pixels = transform_points(data[\"history_positions\"], data[\"raster_from_agent\"])\n",
    "#print(data[\"history_positions\"])\n",
    "#draw_trajectory(im, data[\"negative_positions_pixels\"], TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\n",
    "\n",
    "#print(data[\"target_positions\"])\n",
    "\n",
    "\n",
    "#print(data[\"target_positions\"])\n",
    "target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mvAlKr8Vu-eN"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABFLklEQVR4nO2dd5wkVbX4v6equ2d64s7ubM5kEZSwIEhQkiQVEEFEEUXhPX9iVsQA6Hs+FQMqZngYMIA8QEBAggRBJe0uYQEXWDaxy+Y4uburzu+PU9V5Ns3MzmDf735qp+vWTRXuueeee+qWqCoOh6N28Ya7Ag6HY3hxQsDhqHGcEHA4ahwnBByOGscJAYejxnFCwOGocYZMCIjICSLygogsEJGLh6och8MxMGQo/ARExAdeBI4DlgFPAO9V1ecHvTCHwzEghkoTOBhYoKoLVTUDXA+cMkRlORyOAZAYonwnA68U7S8D3tRf5Pb2dp0xY8YQVeXfD9WQXLaH3q51rFrTSW9fSC4oaHQikEoKk8el6M2EdHaFbO4KitI3EYYTgfqynPuYMsVn/Piheiwcw8mcOXPWqurY8vBhu9sicgFwAcC0adOYPXv2cFXlNUdP13o618xm9Qu/4me/f4m/z9nM0hWZ/PF0ncceM+r5zbd2YeErffzprxv47W1r88czmePp6vou0ARIFKpAF2PG9HDkkcr3vjcWEcHx74OILKkWPlTDgeXA1KL9KVFYHlW9SlVnqeqssWMrhJNjC4RBDsJe6hPdtDR6JJOljdXzIJX0GN2aYNyYJM2N5bc5ATRTEABEv5t4+mnlrru6hvYEHCOKoRICTwC7i8hMEUkBZwG3DVFZNUcQKNmc0tOrBCGU23ZVIQxDgkDJ5ZQwLM9BgSD666h1hkQIqGoOuBC4G/gXcIOqPjcUZdUinie2JTxUtaIt+56Q9O3WZrImCIqpq7uPse1vRejZWVV2jGCGzCagqncCdw5V/rWMquKJkkooCd8MgSUIiA+JhEfCt+FByWHpw/PWgVQKEEft4czAr0FUQ0BJ+uCJVBUCCPge+FWEQIEMNkMgQBaACROE3XdPDVHNHSMR5zb8GiQMAzQMQJS+wOwC248CS4AuoBdYDCzmc5/zufXWyW5moIZwmsC/Ib4HCd8acTagQkgIkK7L8tvvfodEahwNbXuz+4EfAGDcOPdI1Brujv+bku/HtXL2AMDzlOmT1pCoU5raJ7LXXnU7s3qOEYQbDrzWcYY9xwBxQuA1SBiYTcDzIJtVwtBJAseO44TAvyG+JyR8MSGRU4KgTEiITSv6nphjkTojYC3jbAL/psTGfe3HJhALgui/nVgzx0jDaQI1joiH56YDaxonBF6DCCEiigdkMtXeDXA4th0nBF6DhBoSqrX8apq+J7YBhKG6CQTHFnE2gdcgirKlZeGkWAgoaD+aghfbBNxwoKZxmsC/K1G7DoLKKUTBbrzvC3geir/Tq+cYOTgh4HDUOE4IvAaxNQRCEMgFSrmvkHjg+fZ2oSo4u6FjSzgh8BpEVYnNfUFYaR/IT/8TGQb7sR+IgOD8BGodJwT+TYmbdS5QgqD8oCCeh+/Z6iPOJlDbOCHgcNQ4OywERGSqiDwgIs+LyHMi8skofLSI3CsiL0V/2wavug4ADQM0DBGBIKh0CxaR/GpCVZYgdDhKGIgmkAM+q6p7A4cAHxORvYGLgftUdXfgvmjfMYiEYUAYBgQhVVcT9oT8ykDZHITlw4E4H1W0ymrFjtpih4WAqq5Q1bnR7w5sVeHJ2OfGfhNF+w1w6gDr6KjApy8jrF0f0NmdI1e2dFAQKNmM0peBtRuydHRVSgFV6OwK6ezx6M04m0AtMygegyIyA9gfeAwYr6orokMrgfGDUYajQH3jKFZkRvHPZ4SO3jR16SRtUhAEfspjQ1eCB5/o5NmXlY3ddbS1teaPex7U13k88TxMnjma9AT38ZdaZsBCQESagJuAT6nq5uIFKlVVRaSqsln+GTLHtlNX30Rz2zTGTHkzBxy0hq6eoOTbAglfaGkSwnQ9k2aGJFpzTN01lz8uYnHGTBnFuCl7M2bczOE4DccIYUBCQESSmAD4vareHAWvEpGJqrpCRCYCq6ulVdWrgKsAZs2a5Ual20FDQzNTpr+ehuaJ7LXfWsJ+XiP0PGH6vj5hGFaN09LSzJgxo2lqahrqKjtGMDssBMS6/GuAf6nqFUWHbgPOBb4V/b11QDV0VKW+vo5JkyYyceKEAeXjlhZ3DEQTOAw4B5gnIk9FYV/CGv8NIvJhbGH7MwdUQ0dV4sbrGrFjoOywEFDVv9O/v+kxO5qvw+HYuTiPQYejxnFCwOGocZwQcDhqHCcEHI4axwkBh6PGcULA4ahxnBBwOGocJwQcjhrHCQGHo8ZxQsDhqHGcEHA4ahwnBByOGscJAYejxnFCwOGocZwQcDhqHCcEHI4axwkBh6PGcULA4ahxnBBwOGqcAQsBEfFF5EkRuT3anykij4nIAhH5o4ikBl5Nh8MxVAyGJvBJ7BNkMZcD31fV3YANwIcHoQyHwzFEDEgIiMgU4GTgf6N9AY4GboyiuG8ROhwjnIFqAj8ALgLiz9uMATaqavzNq2XYR0orEJELRGS2iMxes2bNAKvhcDh2lB0WAiLydmC1qs7ZkfSqepWqzlLVWWPHug9iOhzDxUC/QPROETkJqAdagB8Co0QkEWkDU4DlA6+mw+EYKnZYE1DVL6rqFFWdAZwF3K+q7wMeAN4dRXPfInQ4RjhD4SfwBeAzIrIAsxFcMwRlOByOQWJAnyaPUdUHgQej3wuBgwcjX4fDMfQ4j8H+UIXzzoPf/hZWrYKTToIXX4S//AXOPBOyWfjqV23LZuGMM+Cuu+CFFyzuqlWW9rzzLK+Pfxx+/GPYvBne8Q6YPRv++U845RTo6oIrroDPfMbifuAD8Mc/wvLlltfLL8Of/wxnnQW5HHz5y/A//wN9fXD66XD//fDcc3DyybB2Lfzyl3DBBZbXRz8Kv/gFbNgAb387PPUUPPQQnHYa9PTA5ZfDF74AYQjvex/cdBMsXQonnmh/b7rJwsMQLroIvv1tS3faaZbPU09Zvhs3Duvtcuw4g6IJ/NuxcSM8+CDceSd0d0NvrzX+o4+GRYss/NZb4Y47LP4++9jx8eNh+nT7fdttcN99ls+tt1qaBQtg1Cg7fsABkMnY7ziv5cvhyCMtriqsX2/H//xn+Ne/CvnecQc0NMDuu1vcadOs7Pj47bfD44/b7zvvtHzr6+34m95kgqg4r/Xr4ZBDLG4qBStWmEC77TaYM6e0ju3thXPcbTdoaSkcP/xw2HXX4btvjh1CVHW468CsWbN09uzZw12NAk8+CYceaj2t79uWyUAyaY0zl4O6OtMAwML7+iCRABELT6UgCGyrq7P0nleaF1jcOC9VS1debiplPfH2lBuG9rtaueXnUFxuImHxq5WbyVg5/ZX7ve/BhRcOzz1zbBURmaOqs8rD3XCgGm94A6xcCTNmwGc/C088YeF33w3f/a715kuXwnHH2bZ0KbS2mkp/110Wd/ZsSztjhuX1hjfAuefC/PnWeK67Dn71K2tcL75oqv6sWdYLT54MX/qSDRcA/vpX+OY3YcwYWLbMtIW3vx0WL4amJvjJT0xbEDH1/MILrZdeuRL22suGBs8+a4Lgxhvh6qshnYaFC02tf/Ob4dVXYdw4uOwyU/PB/l52mWkZr75qgvH00y1dOm353Hij5fvss3D++Tv1NjkGBzccqMYrr8DXvmbj67vvtocerJGvX29DhM9/3sbhYL97euD662H0aAv7r/+Cl16yPD79aWu8vb02ng8CG6cHgfWiX/yiqe+bNlncDRtM9Y7z/+53zcbQ2WmC5cUXrTe+6CLL83e/g+Zm69G/+lV4/nmL/+lPm1D529+s3mEIP/2ppclkzBYwd66dz2c+Ax0dptbPify/vvUtq/fmzXb85ZdNsHzhC5b+17+2YUYYwqWXmv3juON25p1yDAJOCFSjt9d67GzWGrEXKUyLF1uDCQIzAHZ2WvgLL1jYypXWYMAazNq1lsf8+Zbnxo0mGMAETRhaw33pJRMAPT2WVy4Ha9bY8bjczZsL5XZ1We/7wgsWZ8UKa+Rgdof1662Rzp9vavv69RaualpLLme/X3zR8s1mS8uNhxuLFlmdczk7Hp/7iy9a+ldfNa0mPocNG4b+3jgGH1Ud9u3AAw/UEUUYqgaB6syZqpdcovryy6qg+thjqr/4hWpbm2pPj+rJJ9vW06M6apTqVVepPvKIxV240NLOnGl5HXCA6n/8h+rq1arJpOqdd6redJNqXZ3q+vWq556resghqrmc6pQpqv/zP6rz51tec+eqXnmlanu7al+f6rHHqr7rXaqdnapNTarXXqv60EOqIqpLl6p+/vOqe+5p5b7+9aqf/KTqq6+q+r7qvfeqXn+9akOD6ubNqmedpXrkkaqZjOqECarf+Y7qvHlW7rx5tj9hgmo2q3rEEarvfa+la2iwfO691/JdscKum2PEAszWKu1v2AWAjkQh8MwzqlOn2sPd0qI6aZJdqvHjVUePVvU8O15fb9vUqdYAR4+2OGBpWlosj6lTreE3NalOnmzHx461Rg3W6BsbVVOpQrmtraoTJ9rxCRNM8BSXm05bOhHVMWNUx40rlNvcrJpIFMptbi6UO26cxRex9A0NJoimTrX8R40qlDtxou3H5dbVWfxq5U6erHrNNcN95xxboD8hMKINgytXdvLlLz9Ydbvxxn9tPYMdpb3djGktLXDggfDe91r4aafBEUeYJfzDH7apsunT7XddnRnsTjvN4p59tqVtabG8xo6Fvfc246Dn2fz/295mav0HPwh77mkGuAsuMGPfwQfDe95jeZ1+Ohx2mI2/zz/fDIe77GLlJpNw1FHmbwDw/vfDfvuZ8fI//gPa2mDffeGcc8xwePLJcOyxpsafd54ZECdOtHwbGmyq8IwzLK8zzrD9xkY7PnGixT/vPEt/7LGWn4jlv88+Q3dPHEPGiBUCa9b0Mnv2Gr7xjTlVt9tuWzx0hbe1WQNsaoLXv94s8WBGr4MOMiFwxhkwaZJtZ5xhYQcdZA0DLM3rX295vOc9lueuu8Kpp5oQOPJIa9i+b4Jjxgyz/p95plne993XBAWYsDjgABM0Z5wBEybAlCkmHJJJm/s/+mhrjO94h80INDdbXq2t5k/wznfa8aOOMiu/78O73mU+BuPGWdz6ehMgJ5xg5Z5wgu3X19vxsWNN6L3rXZb+0EMtPxETQnvsMXT3xDFkjFg/gU9+8hGuvPIlYFzVNOecM5lrr913aCo0Z45N1zm2jx/8AD75yeGuhaMfXoN+As3Ym8hv7mfbbeiKft3rbOps8mRTz2PPwN/9Di65xFT8xx6zocERR9jv5mabJvvtby3unXda2smTLa+99oJ3vxseeMBU6Z/8xKYcUymbj3/nO82XYO5cGxZceCHccovldf31cPHFpuI/8YT1/MccA488Yir8179uPgciNqV57rmmWTz5pGkfZ59tvgaeZ1OTl19uvfs//mG9/YEHml/DmDE2rXjTTVbuTTfZfnu7CcYDDjB34n/8w9Jffrnl53nmHXn22UN3TxxDxgieImwHGrBlCip54YV1XHXVfM47740kEoMsyzo77UHv7bUptVhLefppmzfP5awBxtNyjzxiYQsW2DQaWGNdutTy+Mc/bFpv5UoTGGFoPgDZrP1+7DGb19+8Gf7+d5vWW7LEBAKYA9DLL1v8f/7Tpu36+qzceMpu3bpCucuXWz3+/nf7++qr5ocA5tTT2WnpHn3UpgQ3bbJ8s1mbjmxosLhz59p+JmPnsGmTNfhHH7X08+fbcAfsHMaMsSGD47VFNWvhzt6qzQ584hNmdO5/e0abm7+j3d2ZQbSfRjz1lFnuPc+s4aNHW6GjRpmFX8SOp1K2tbdbWFOTxQFL09BgebS3m7W+vt4s6mAzBy0t9nvMGLO8JxKl5ba1FcptbCyUm0xaubGVv7nZZhPictNpm2Fob7e/6XThHFpbLX5xuclk4RwaGwvltrXZfnwOyaTFj8+hvNxf/GLw74Vj0OC1ODswbLzhDdZ7Tp9u6vBjj1n4nXfCd75javnixWYEPPZY+93aap598dDh8cct7fTplte++9rbgc89Z8OBP/wBrrnGjH3z55vx8MADrRefNMm8CP/xD8vr3nvhG9+wnnbpUjMqnnyyeTI2NsKPfmSefiLWe3/sYzYMePVVG4acf75pMb4PN9xgKnxDg2kup55qBr5ly8xAeOml9tIT2N9LL7XwZctsGHLaaZauocHyueEGy/eZZ+AjH9mZd8kxWFSTDDt7G3GawIsvmkNOOm3OPocfboUefLDq615nPfYxx1jv2N5uvxMJO3bwwRb38MMtbTpteTU32/z6W95iPe7++6u+8Y3Wy771rTa/39pqcevqVHfbTfXNb7a83vQmc/5JJq2s0aNtfv7oo62nf/3rVWfNsrhHHKE6fbppEsceaz35tGnmEATmtLTvvlbuUUeZX0Nbm8VNpVR331310EMt7qGH2n4qZcdHjTKfhaOOsvT77mv5geV/882Dfy8cgwb9aAIj2CYwjHieGfpErKdubLTwhgYbN4vYWDgRXb6mJgurry+MpxsbLa2I5eX7Np0Xj6HTaRtXx+kTCYvT3Gzlp1KFchsbzbYQ55VI2BaXm06XlptKFc6huFwRixe/DdjUZMeCoHC8+HwbG+19As+z475fWW4qVZqX47VHNcmws7fqmkCoEPSrCfj+PB079vva3Z0dVGmpquZu292tOmOG6he/qPqvf1mhDz+s+uMfW4+4caPqiSfatnGj9eI//am574K5/H7xi5ZHd7fqfvupfuQjqsuWmdZw663mdltXZy6355xjWkRXl3nffe1r5rkIqo8+qnrFFTYW37zZNIBTT1Vdt87sENdco3rffaZhLFig+pnPqO6xh5W7996qF16oumiRaQ133qn629+ahrJmjeqZZ5r20NFhWsE3v6k6Z46VO2eO7U+YYC7Khx2m+p73WLp02vK5807Ld/Ficy0eoYRhqN3dOe3uDrW7W/PbCK7yoMNQuA0Do7APjczHvkJ0KDAauBd4KfrbtrV8qguB1QqL+xUCZ52V082bezUcCn/1J5809V3EVOHGRiu0ocGMe7FRLJGwLTa01ddbHCi4AceGO88ruA6DNaJ02n43Ndkx3y+UW1dXKLex0fbjvHy//3Kbmqxcz6tebkNDabmJxPaXW3wOxeX+9KeDfy8GifXrMzpmzH3a3LxGm5s1v/3618Nds51Hf0JgoIbBHwJ3qepewBsjQXAxcJ+q7g7cF+3vAB3AKmBF1S2Z7KC5uQ776NEgM3WqLQXW3m7z4v/1XxZ+0UXmQtzQYI4x++xj2w9+YGFnn22vFYOlOfFEy+PHP7Y8Dz/c5tZ934x3559v6vV3vmPuuTNmmJGvrc08/y691PK6+GLz2Gtqgh/+0Ix9++1nfgb19eYX8OlPm1r+9a+bsXL8eCt34kTz6vvGN0yt//jHze03lbL0s2aZR+GVV5r/w2mn2evOYH9PO83Cr7zSXIYPOqjg33DeeZaf59l6B2996+Dfi0FC1aejYy86Opro6CC//fzn9vZ1LbPDNgERaQWOBD4IoKoZICMipwBvjaL9BluA9Avbm//MmT6HHeYD2arH99hjCL9zWldnvvnJpDXIadMsfPJkm3f3fZg5szAOnznTGsLYsRYHLE1bm+Wxyy6WZ0uLNXQw1994bD5zpo3f6+stbiJh6xLE5U6ZYj4JnmfH02kbr8+caenHjbPGDjYbMWpUodxUymYupk+345MmmZ+BiNWlqcnsDbvsYuc1ZowJLLC/y5dbfXbZxerX1GTp4nJHjSqU29o6VHdkkEgDfknIo4+aq0ZNC4Jq6sG2bMB+wOPAr4Ense8RNmKfIYvjSPF+f9uIe4twzhxTgeOxR/xbpPT39h4fqrhDmdf2xP3hD4f7zvXLunUZra+/R2GVQlgytDz44OGu3c6BIRgOJIADgJ+p6v5AF2Wqf1SwVks8or9FuM8+Nh8/bRp86lOFZb7+/Gdbbae11Tz4jjnGtpdftl7+8sttcU4wb75PfcryWLrU8jznHJtPTyTMvfjqq62nfu45U/cPOMA8BSdOtNV7/vY3y+uuu+C//9u0g0WLzFU5Xv24sdGGIzffbL3z44/bCsO77mrl7rmnzd/PnWs9/XXX2epC6bT5J5xyig1FFi82TeYrXzEXYLC/X/mK9fiLF5ufwKmnWrp02vK57jrL98knbXgwQhk1KsGCBUdwzDFZYOVwV2dEMZApwmXAMlWNPGm4ERMCq0RkoqquEJGJwOpqiVX1KuAqgAMO2E/XrVlGkMuidiyKVTre17J9tNweUMU+IJIPL+TbHxYvuXoNY6+7ztxkH3/cVvwBWwZ89Wpz2f3JT6xhgP3u67NGM2+ehf3qV/Z70yYbT69ZY/tXXWX65y23mKtxvNTY88+byv+jH5lb7z//WXAF/sMfTC3v6bFx/tKlNrz46U9tSHHPPSaYVG258TlzbJWfH/3I8njySfjf/7Vy//QnyyebhZ/9zARJZ6fF7e6Ghx+2ssDelVi40Fyef/QjC9+82dJls7bKcDpt+V59ta2TePjhW7nGw4PnCZMn1/PRj7ZUrIAWj6Ri1q2zldX7I5Ew2ZhOD349h4MBvUUoIg8DH1HVF0Tkq9hwAGCdqn5LRC4GRqvqRVvKZ9999tIbf/8Durrg1TXtoCHWIAuNWpGtCIFC/PZRG2lp7kQVwhJlp+xci3ZVhVdWTCZQj/qXFzLj8xexCwvZTAubaGUXFrKE6dTTy1jWsIDdmBx9ZnE5k9mNBaxmNH2kmM6rLGQqrXTQQhcL2YUZLKaHNGtpZzcWsIwpCMpklpOd0oa/oQvJhQQTRpFYth5tSRM21uMvW0c4qQ16s3gbuwimjcVbuRE8IRzbir90DeGYZjTp46/YQDB1LLK5G6+7l9zksSSWryVMpwhbm0i8sppgwhgIQvy1G8lNG4+/diMEIbkJY0i+soqwpYmwoY7k8jVkJ49DunvxN3eSnTqRxIo1kPDJjWkj+coKgvY21PNIrFpLduokuOQS9Jxzogu7vQbb7Ym/5bjb8kSrwiuv1BOGlXm98oq9vd0fqZS9lrHHHlpmBhkCI/Ug0t9bhAMVAvthtoAUsBD4EPZm4g3ANGAJcKaqrt9SPtMmNekvv3kIi16ZxQWXfHOH6xNz2YU/5/2n3E4QQGdXjjAEzxfq67zCfVIllyukyeZ8jjn3D2zubM6HLWA3fs/7+BUfYiG7cDCPsz9P8i0uZiIruInTATidm1jJBD7PZ5nHnjzKmczkPs7jJt7HXezGAmYzi8c5mEv4b15lEu/kNtL0cFPyDBbe+jHG/uh+6pdtZPU15zPxHd+j56wj6T1uP9rf/nXW/99nST65iMaf3c3q+7/O6I/+nLApzYZvfpAJb/kCm75yFsHkdsZ88ApW3Pstmn9/P/UPPM2KW/+HCadfSu8he7P5vJOYfOxnWX31xXgbOhjzlV+w9MGf0f61/8Vfs4GVV3+ZqcdeyKYPvoPuw/dnymmfZenNP6Dx73MZde2fWXzPNUz+yFfITWhn9SUfZeZRH2Tl1z5F0NbClP+8hEX3XUd29KioBQbYSLNUiJfub/m44SFihrxQtdC4w/K45XmXolFexWQywjHH7UdHx44rw7/8ZcCHPhSX5lep/8hiSITAYNHanNZdp13F6nXHsnzVxK0n2ArvOv4KjjzoJpIJoa8vtH5JhKaG0gchlbSb9vLS8Vz358NYuOw/CMM0r+N5rucs9uQFNjKKDbSxJy+wgN1I08MEVvIvXsd0lgCwhOnszfOsYCy91LErS3mBmbSxiVF08gJ7sjsv0U0DK5nA3jzPYmYgKNNlCZnpY0is60SyIbkpo0kuXkPY2og2p/EXryaY1o70ZPDWd5KbOR7/1fXgCcGENhIvryQcPwpN+PjL1pKbOQFvYydeZy/ZGRNILllJ2FBP2NZMYtEKcpPHImGIv3I92V0m469ej+QCcpPHkVy0nKCthbAxTWrJCjLTJ+J19eBv6CAzcwrJ5SvRRILcuNGkFi4jN74d9T2Sy1eR2WUam9+/B91vm0wyARUNopoSpsUNV0ujqI0yejNKJqv4vpCus/tnlqZC7FBLs4/Xb81npSZEYmbP25ev//SjLFyUrqoJbCtXfPVW3nPq0/ieh3gpPM/H8zz8RB2ILcAqWE8TlgmJSiHooSWarQdaeF5VBHPw9YEAVCk16UmZ1isoPp+75HDmPd8OwLx51YXAiHAb7ssqC5aOoaNz4AIA4KUlvQThZhIJIQgU1cgjNlW4SELB63f5qjYWLG0lvikdNPMARzGdJSxmBs+zN3vyAnM4kLGsoZ21/I23cDL2stDfeAu78xLz2Yd1jGFXlvIIb2Fvnmcv5vMARzGBlaxkAo9wKHsxn6d5IwlyTNUl3LA6w5syIaPCkNtX9HBmoMzvyLC4RzkduH1ND+ODgP1D5fqVXRzXF5AVeHhlF2ep8sjGXjo9jxOBW9Z28bq+LFODkFvXdPCOXMjKngzPhl2cDvxtUzd1qhyqyi3rOzi4L0s6DLl/3WZODZX53X2syAWcBNzf0cWEvix7asgdHZt5Sy6gR5U5HR28Q5W5vT1kPOEw4L7uzWx88VUyDX323RRPSuSAJ1Lad3tQHCJeWd8e7WSyShAoCV+oi4RAWdZImXlbhAr/keLdxYsns+Dlhv4foG1k4/pXWbH0OWv4fgLP8xDxCNTH9xL4vuJ7mq9k6flKWR1LjyOCSHmj9shkhWRC8X3KjlseMT299Vx/5wn84x8plizb8nmMCCGQyylhvLz2ILBgSS/LVnVsc/xstrNkfyUT+AZf4hRu5e8cznW8lw/ya37Fh9iHZzmQOVzOF9iDFwG4nC/wfn7HnZzE8+zNmdzAj7mQs/kD41nFN/gSR/EAczmA7/NpzuOXXM9Z1NPLcdzBLxpbaQlDZgRZftzcykk9XTxaV89f0w2c3t3JHxub2S/Tx165DD9raWOvbIZuz+MXraN4V1cHf21o5FU/wQndXfy2pZUzOjbT3BPyi7Y2Duvq5pm6en43qpV3bd7MHa0ttAYBB3X38JsxbbTncozJBfyyfTTHb+7k8eYGnmhKc+LGDm5pb2VWRzfT+jL8btIYXtfVw9pkgusnjubEtZv4+5hmOhI+b17fwS2TR7N8cUjvkrUgkExIScNLJDy8on3fl/xK7gAJX/D9QgTfszhhqHgi0QeZIsNtAvyizP2EmNDJlyUkitwBfE9IFOW9YGnvNj8bW2Llqk3Mf8lmGlJJQRBUle7ekGRSSCU96iJt0/PLrocvpdfDK933PEtTTkdnSGPao66u9PpJlEfM2vWj+dq3L0W1XFBUMiKGA4lEnXreTWSzbx+U/NLpS6mvv3ab42ezu9LZ+X7gvUA9+zOXxzkYnwCN1CyfIK9ueYQE+HiY4NrScUEJ8LeYV7EZNMAUvnhuNVL+8sdDCvI+/h3H9aKw8rhbO14tr20tl6LjX25o46q66ovAbI2tKuWDONzOZo6mo/OXA86nsfHz1Nf9HwDJhIcQXz97LiT6B1U0Ha9UO0FK9ystJJHmoIW4Ui4EigRfLjeOlxc/TKlz1GtuebHhYwG7cTx3s4KJ/I738wFMoHycH/EDPkUnTZzMHTzGm3iMN3Eyd9BJE9/n03yCKwE4h9/yO97PCiZyPHfzErtzO2/nDP6PAJ8v8z98jcvIAuc0juWBRD0veEne3TSOteLx+1QTFzaMQYHPNozm6rpmNotwRtM4nvZTPJKo46ymcfQi/LC+hS+n21Dg/MZ2bkk2sFx83tU0jqWez+3JBs5rbCcEvppu43v1rfQBZzeN5eFEPc/5Vu5G8fhNqolPRuV+smEMv0k1sUE83t00juf9JA8l6jm7aSx9wPfqW7ks3UYInNfYzu3JHVexdWubDt4WDlK/p6qEUX6ZbEhfLiSTC8nmIJuDTE7py1l4b6Z06+kN6S7eekK6uku3zpItoCPeugI2dwVs7ihsmzoCNmwqbBs7cls/gYgRMRwYbkQ6SSafJ5u1V3s7aOF+juYmTudhjuBB3soNnMndHM8MFtPGBu7naPbjKQDu52hu5N3cyUksZRo3cCZ/5VgCfDbRyv0czc28iwXsxl85lhs4k7s4gRQZ9mQeDybmMikMmKABDyXquS3VwAOJNHMSKW5J2u9lXoK0Kg8l6rkjmaZHPB5K1HNLqoF7E2nWej635Pp4MFFPQpVVns/DiXr+nGzkeT/J3xJpbkk2cF+ynmZVpoU5HkqkmZnMMUpDHkrUc2uygXuSaZ6Jy03Ws06sh3soUc/tyQbWi8dDiTS3phr5azJNhwj75Rp4MJmmo2KMWjto/r+txNliwDaVsk1sz+h6xAwH6lLXk8mcTC4YyDsB1m80NX6V+vS1haBtSplk06aHUW3dpjTlUcKwnh3TWftoa5uFyBZsGP3UZ/jv3GuTTOZouroGPhxoaPgcdXU3DkKNBp8wHMemTY+wLcOBEaEJNDf6fPD077Jh09/5zc3fG2Bui9h/7yz77jkGFLp7zT9aRPE9IVQIwlIfAQAUMtlTCEIpmV4KQrNOl9OXC4mGfqgmeejxu8kF2z8eFoGpE1P4XkH4BfGcV1RsvKpCWXUtXhlhWV1zQZVCy1ViBdXKrqNqZ1KlLmEIeGpTftExrRKvIiuF3GDp5o4dZkRoAjMmN+v/fuNNJBPjWLzsoH7j5f18tphbJ+PbF9DWYlbb3kxkpBGzyFYdF0a/Q7XpxJJDWt3dOBcUFgbyfY9Va45FtXJlna2rgAGjWu9BJJcvry9rrjOeZ1byaqpdWH4OUeblbTkICkXmAiUMJTIiFRKrQliUWSx/ysvNj6ujuHG8IIBsYOZB0RBPhCAsmvfXqB5aSAdWZrZYaMVCLyzch2xWyQWKSKn1W7UQL78flOYfn3PxGyx238bT1/dmbrrnc2Sz2+/760mOYw/7FqnUI4gsKapPVKfo+ojYXGYupyXXr/gciyk+h2KCUAv3Qir9IELVimdUtZ7e3rexbMXH6O3bJU48cjUBz4PGtMfo1o1Mm3RfhVIdXcuC9bRsztnCKJligRQiQiZnFnpPhERkwa1YgiDaL6QvNdNWSxME9kCJRCts8c8qdVL6MimeeWEP60HLj8cPMAXBp2oNXwQ8L8T3+lDto9DkMlG8EFUTHLvN6GVsWxYpd7ghfvDMbm1z7ja1lEqWxgnLHqJQqwufCoERPfy9GUEkxJMQT0x7oayBFhpsoVHkqjz0uZydR6jQ26cEQRjdv8IVDivyhFwQ5n/n8wpKz8MEYQ+53IOsWnsCmWxD5fOwFTwvxwlH/g3f67bhY1yfSGvMBZp/ZjxP8h95VjQvuINAK69v0TmUXI8obiwYy7WsULXi/gHkcvez+JU30x29+/KPudXPZ0RoAm0taT339NfRPipJQ9qvOJ5K2hxzfLNSKSlr8HaxUwnJT5uI2NxwJghBrYGnkh7JhDWCOHnx/LUnNudcMv8alZss+7aB5pRcaE4f6Xp7QGNhla+TeCxfNZFjP3g1O2YvyACLgFej3wG2qApAL7AJgJ9+dQFnnLC25Brl6++ZwFSF3j57wHxfqE+VO9OUJ+y/VtUEcE9vQDLh5Rvqtma3Je1OgUxGbc7cK/UFqMineE5NyYvDuLuI84+jhKpks5r3TxAp1fji61HSPMorW3S/8/EU+rJhpMXFz1JxvatlGIcXl6klBcSCNBeI+Ul4VBH5/QzBIuHRvP+tI1cT6OwJ+b+7NuB50q9U3vpDVdoCq9yv8ijVkuH397CVC53isoQSwRHT2XUBmzafX3lguxmPrbS0AZgIrMOEgPH9a1fyhzsWlTjbxPiRYFBsjO8jFXPUnlCSVqTgTVmSl+eVXiu/cB2CUEgWO+lI4U+sgcUkkwVBEc9tVzgLRdc0m1P8SAtIltiMBQ+JnHTiE4nrKfi+V1JG7CwkYMMTBfECkr5vQgAhlSxcFynKK+EV6ut5gp8oyisilbCOKcQEonU4XonTlOcV6lHsF5DwpfB8xsOesmc51IJGkK6z8/akqB4S35Oi6yHk42ypsx8RQkAVevqK9MdhpLw331K8Le0DdHWn6O4eNQi18iHva175osra9Tm6e/sqtKPKehVpK1sQakQPezkV7rpFD7KqRD12ldqX5eUVNariBuIVVVbEygvCqFyvMh+hrMHkNbpCw4vLyJcJhaGZhCbYosiJInf+4vQV9apybYrTZnMaaQGSH9pZWiFZJASKr0dxw/XLr6EU6hyGSjJhArP8noEJz9JrYWrvlhT+ESEERhLlhsH+I249yiB6Qm+RvkwIspMKc+wwIuS1iq3FKxfSEoWHWhBs1Tqe8iDpL2IRTgg4HDsJVUpnQ3ZOqVuNMSKEgIjNDhSrWVV7ZK2cz65qBykbWfSf19brFoQ6aG6mDsdIZEQIgVEtKc44YQKpZJA3FPXnbBKUTaFUa6BhSMm8cBiqvY0WG63yc7XV0pbNK4da6pQT/czmQoIwnoUo1EXDgsB59oU6nnupNP964D+BW4DF2Aj/o0AdNgdwHXA+9i3mDYD5tXVihsAQW8qx1ANoxuQ6xrc3ROdUKvEK51g2zVR+4lp07kWnammqS8H4WLEx28K1Il5F2jLBrFXmzfPnE7+EQ1F5xWmrzLdXF/pavFu1vOphpYFVr0aVOlSNti3DyJ3c6YwIITBmVB0fPnMyzfUZ6usK1sxqN7fcwyyMnWHK5k3jtL0ZyOVCNAxIJqXUgaXI6aVEaASFB21zJsumTK7w2pyACnTnArJZRTyhvkFK0sZV9G6pY10kBNaOgWQWpnXAZ8bB0g1WtxTKJwiYjMfzeDyIcmnbZib31rGox+NOfNaynhQhLaSADawnR6bohPfdo4H9925FQxNOxVcolytcy1ygBGHl9bJdJZspDQ4iARhU8zoktlZr5DtQMApmc0WtXqt7LebCUgeXIKj0VUBtilAjvxDPU4Iq78WUO06FWmUOntL7GpdZTvFcfFwJLcq8P/N1sYJa7ktRkraa70XZTjYIq3qIxsbBkiF+Zf9Umq4oow0bqkRghAiBbLaPINNJXUuSdKpgOa5AyhdiqE5xjN6MIp5HwkuWvGrZXxnlQd98cSlff2U+jIkONgKjQacW7U+uXo/LFh3Pb6PFhw/6Cxz8BPzXJTBtMfzpFLjhHuhFmcoirmAc59LKKwLd136OzE0nMPPX72IZu3Igf+MwpnAlRwPKCVzPX1mVL+f4w1p579tz+cVTqqGY951i1uikV368qCvfDuJHrDcDCZ+S9/grBU1pYLFA7m+41pfBXL6j9QSq9pJlWmNlQ47DoxqEkAttjdZkovQVXEtb6oQUFmkRsXNUeb3DyEPS/DGEIBcgonh+IVKxA1ax92Q2W3RlYkFeItTMgaq3LyQXKnVJz6ZTVenLFvJXKtP2ZTTvnfiRLy2qcvEY8BqDnwY+EpU/D1tjcCJwPdZs5gDnqH2YpF9amtJ6wlv2oCHt5adHbMqoojz8avPXWxAOsVU13hKJSl8EW7SidBoo4Qv/2HUJL+Q2s6SvG9qBNkxvrwOmAmOBpmi/CjMWzeCQRw/hD2f/gbkHCA8cdSv3HXMNn//Ozfzh7C8xYWWK/7rkv/knPaw56I+kW9Zx/H2fJdj3RZ6d9HdeTS3jxD9fyhxWsn6vxwj2e4zj//h15ula5s98jCePuwaAvXbpZXx7zuaby6bx8o4w2AMBtuBHQ13x+Uq0Uk3phUn4lZZqT4r8KPLTg0pfxhbWSBbN2+fvYdEUWSx7SvwUiubJ83WK/vb0WENKJMwPofo0aPk9rd6RxGnN29Dcs+tTlm9MwcmovCYF8k2mSo8cqtLbp9GiKCZwq7ewImFYxT27HHMUMu3MpgiLh8zFtrTKoVgcMu0ttw+us5CITAY+Aeytqj0icgNwFnAS8H1VvV5Efg58GPjZlvLqzYTMeb4rcpqIbkLZvHFMeW8O8TxrdSEQC5X4IfP9SiFQ7pcuUbxHG9fR25gtrLqQAopfD4gFQj8snrmY7oZurnvvdZxw1wlsaGtk3r5trJj4Rxq7+hi9fgIewuE0MLdvFJ09OQQhMW9P6rOv0DCpC4BZTOSV7CT+1dXEn1K3c3jmUCbldmNsxxu5PXk3c/+VM6ecKo0pdoSJewmieMUegyJS1VErdnQpWexCilb1icOjYVrCL3Xtjef+S+fciy98lbn+onIEWyPS801QJHypnEMnKqNsgQ2QCp+FOG3Bzx9SCUpWHYozKBU2pV6m+VCP/MXJXwpVsjkToLGnY/nlsmtT3THOk8p6e1Koc6gmEP24fRSVX81Ho5pPQzk7rAlEQuBR7BuEmzFb14+A3wMTVDUnIocCX1XV47eUVyJRpy0tg7O+4GChKJs/8irhqMge0I457qWw/anAFEwT2AISCg3dDTx8xMPs/9T+rBu9jt0W7MYNZ97AcX89rt90GTLkCGig8ILLOlnPgS1v4dquX3Bk7s300Mv+rUewTtYTSD8D9wGQ8PsTrZV4CaX4LUKoMpfdz5R11VUIYu0uDKP1Nk2YVX/QK7VGKF2eKy9sIkIUj8iZp6JsKrwvi7WFmPIl0vLliiCiVmMpXeosJpGo9Ew1TdWz+EWHrOzIzhEJCd+DVJGHkoh9eS56O6aQNgUSlfODa2YPriagqstF5LvAUqAHuAdT/zdq/GaLfaCknxFzbTB9yXSe3edZ6nvrARi9fjTLpizL7/fHFfU/4Y7k3TzU8Ze8ljNa23h+0+PUYf6z9dQxd9NDfKLxIm5O/XnQ617t5Z5+CaAfu/kOUp7XwPPuV6Btvylkp5ZbpHDtUH5bK2aHl4IRkTbgFGAmMAkzkZ2wHenznyFTHfxebKSwZuwaPvSrD7Fk+hIAOps6Of/q85m37zweP+hx3vO79/Ch9v/H3xOPAKaBfC79FTw8Lu2x77helv4mv0v9kU46+UTjF5jv2wKnWbJ8ruErPO7383qYowTtb9Oh3cL+tnDbtnjoUr5fsQXVt1zQz7oSEQOZHTgWWKSqawBE5GbgMGCUiCQibWAKRJ/pKb8hRZ8hq6+r08njkySjN/rKLcBVp1u0yuuvVd9/L1i9txqfKH4UrdPrZ2GN7SDwA9a2ryWXyLF80nKe3P9J1oxdwxMHPcGKiSu48fQbOf6i0+gjQ6fXwXP181jlreaQvoM4IjiExxseZYW3At8TZieeYJ2sI0uOdf46nqt7ljsb7qRHeklG51h1aqn4V5UOdZv62B1INwJeUHVsAwMRAkuBQ0SkARsOHAPMBh4A3o3NEJwL3Lq1jBrTPqcdM5r2tgSplEcmU+ql15cNzIJaFBaESiZXCAhDe18+RtWmXrL5d7uFTLbQpBXI9JU+pbnQ3gUPI6m5ts6ng1zJVFZVLbU/NSwUpr4ylfuOuY/QC7nqgqu47GuXsWT6Ek695VTuPe5e0j1p/tBzFa1hK/ManuJj0z/E7QvuZ3wwgaV1S7hg6nn8cemNPJV+kq+N/gp/XfAQvudxb9PdfHriJ6gPlPqiJaSKFxEBSpyXlDD/ikEhrP/pueL9alNzxXmHGlK8ui5AoNVXZdpWhl2G7KjAfI0x0CnCrwHvAXLY58k/gtkArgdGR2HvV1sVo1/2mNGif/7F4YxuTZBKelUeympvTlNyR6odt3nmEPHsxY3i2YZYDazMsqAJZJI5vr/wJa5YuMAMgx55PwGmYu5+TdikaBU+/+3Pc9G3L2LMujG87Z63sdf8vfj4jz7OUQ8cxdr2tRxz3zH86oKrafz5fyPpXrKSY7O3ibruUSQ9D2/FWILzLybxkyvQ52YS/vR02oI2ur75HboPfJIOr6NygYmwtGHnVwFSm8uPnXqSCc17OIbFEiG6BmFQKijiBTyKL1m8MEaoQmdnnxmsigxqpoqWaV9qzlvF9c1ki66+khfuoSq9vfZNCommcVWj71TEddVYcBfKyQWlPvphUOrAFITmhBRotIALNv1Wfn7ZbFjaGWXKvEej65or7owU+vrC/AyDCCWdU/G1KXFwCksXBglDLfFoDQMIchCKFqY6i85Rqe4BW7zC00svvzz46wmo6mXAZWXBC4GDtyefzu6AR57qZMyoBMmkF1lyS6c3yl819bzS6cJyK6wgeL6YEIjm/euKp8WotAD7ZVNsvie8KTGWz4zzIR2VnwSykFsR3eg6IbXJEnjxnFeUxZsnrMM762Y2AKc05pg4ZS0tp97Nf3aPJlg+it2SCdJn3k5f2xpzJwSaxKNLNhL4QmpKlvTHbyEzcxleUzd1vtAjELz+RZJt3YymitlZi06gEGCva2fsh+8LqUTpmLgim7IHKqwiiPNj3hB6enwSZdNtxZ6b+fhQujyZlg7LCs451sB7+2x9yHiRDqV0eS3FllTTkgZU7kFIXruL65XLWcNPJOy+VvNQNe/KQmB5w40rXFz/UM1BLeEVZkeqrgVZds7FDkr5sCKtLQztFWWlsPJUPn50Xau70BeE98cue7kyAgPUBAaL5qa0nnrcHrS1JPCjBSgSftFrl0L+3ewY3xeSRXP2npQ6fcTTLZlsmG/wxQ4yRPGLm0syKSXOMUnfK9n3i+Z2+zL2VCUSQrrOj+pQmAKL30PPTyFJNB9fNude+hUZO8fOHiXh29RQ+dRUtXfNix2EREoFWfxb1XrceMHVZELyU0flFPKqLKcYL8o8VCXbZ1NUiWSpoK3Io1pZ8Vx3/nhk38B649jpJp5bL5mZl4pvVVeuCVHmTBQ3sGyg0UIoUr1usuX9wjkWCz7zckwl7N6Wf3SE/JlVXIXSXS2dFQhiIaD2nPrFX38p+lnN1T4WZe0H3zZyVxbq6Q2542+bhrsaJP0tuyX392p2aUMvik81B5PSVlzh0FL0AhVqPZVEPZVIdWeZ8rnu8k9eWd2lbNEMKVk9yI6XJkqWPR2ppFeSb7w0W0gIASSTXn7VHYsv+Y++Wj2jOuTPXUgkC3XyxDQUr2zSyk/YG6a+L9Qnyc9piZStLJSvd6lWWO7EBAUtKBUJARHynwzr73xTycrr6nllHodq6yKm01ZfE7iF+CKSX+0pCihZoSm+TiUelZ4Jgb5o0dx0nVfagRUtSlLhaFTkINUfI0IINNR7vPOoUbQ0lX25VZVstnKgkwlK1b9cUKm+xp5b+bcORSrGp9mysVrx2A4q/bCDYjU0erlGIe+tFvajLubKVetASwyKJWM5AY1edCmxzWk0bozU2pCCSln8XIaR5M/3IrHOLoV65oOiJy0WOHFvWJSkJPPyB6nkuEQZaMEOUE1oStmOlHXB1URwcYMpX7Kv3/c/yoVxWbz4VPKaU5U4FR6Uvlf9fMrDPC2YSKXS27D8Gy1lXtjVOwqNXmSK7C6lQ+OC0KgQIP10XMWMCCHQ1JTi0APG0twQlPU+lUtoQ6mxA0r9o4uSEoZKbyZWuyvVxvJ05ZbsIKgsBwrGoFxoecd1Ln8Vt798y6cmyw06fRnNDxXiG2gqbKkBqlzg2Hi77Bzjc1DIarF2UmpUKnbVUKq/YWf1LB17xwKjN5PNL6lVLFxNla02Jq7+lmj5SsZ9Wc0/yPG1qBjzl+VVvlKyqlbkG69WLEUux+VvGZYv7Z0/oaJytKIehcDYrlFyD4uEP5EXYMVzWeY3E98/VFGpomFGw51Q40a/BcldhREhBJqbGtj/jTNJJ7tprC99iqst+lkhjauOu+yCdPUKniiJhFJXphJWfjVrG+0jaka2MDT1vT61lXRVjGz9laoKXT1CwleSicIDhJZqP2YsqtK4yo1bkdahCj1Z60nNyFo6XVpiaVat6lxSvlZ+vPBlGAqbOnvNDz8hee0tNnhls7FGVUiczWmREC7UsWTmQGFzZ0jCjwzFfpy2tJEHQVixrHhx/YNQCcryzQUmbO3txCjfLCVks6Xnm61STolRMJpiDgLF88xWUnxdbVnyQnzV6gIylystN/7ugEYSQMulDwW7QTnVvm9QzogQAnUNY2ma/lF6e3vYWNJiypViQwjLGn01lx5FUGiO5aYiZd2kFC/OIWX7gEhl/Hyb9L3oeEh3VEc7Xqkze2XnJJS+FC8SlJyPn/ZBAnIl9Qkr8vGobKkJKc3b4lidGiSBoNH1Kzzx5udeyMviVOYtUsjL4oXRX4+cJiHsQzVDGGQIg4AgCAjCEA0yBEEYLRwSkgsCFHN/C8Mw6kHNEcT3NC8wbLrQJ+mH+L7mx7dVP+NdZZn4wr5QPBEUhEImZ8KrLmXCNo5XYdMpH6qUD0XKep8whI5uoaleq77stiOYcLSOx2wXVQzE9OPLEc16qML0t86vmv+IEALJZJIZu+xBGFbpfrbZPaOasKD0JlUbM5Qk6CePqvGlaH9LdawchpTHr3zZNDYPb2++W847TiFVhevW6hTF6WcMrhp9+kRDIBdpLfEqTWFBpY3CcrksYWhCIgxiYRCSy2XJZbMEuRzZXA7tzaJhSDb6zHtcYMkYGy3VDivGwVrxGKh6SBiS9RTxNJ9P6XmVdQJSep3Kr6Og4CtBi8cmCaN6bSnP6p1cuXAXQjQpaL2AKBmCig6qWn2NYi+76u+XjAghICI0NDYOdzUcg4KyLc7WQS4g1DBa+i2MeqyQXC5HkMsR5AJyQUA2k7Xj8XihH0oacJUeurKxmRBAigVbuebXjyDs93ikfcbv/mqpILdqlV+bah1PuRCI6xi/FF7FWCFavb5SrjVXMiKEgOPfCYFqTkxl+Al/G2I5dga1+0F5h8MBOCHgcNQ8Tgg4HDWOEwIOR43jhIDDUeM4IeBw1DhOCDgcNY4TAg5HjeOEgMNR42xVCIjIL0VktYg8WxQ2WkTuFZGXor9tUbiIyJUiskBEnhGRA4ay8g6HY+Bsiybwayq/J3AxcJ+q7g7cF+0DnAjsHm0XsJXPjzkcjuFnq0JAVR8C1pcFnwL8Jvr9G+DUovBr1XgU+wbByPq+mMPhKGFHbQLjVXVF9Hsl9pU+sOXGXymKV/OfIXM4RjoDNgxq/OL4dlL8GbI1a9YMtBoOh2MH2VEhsCpW86O/q6Pw5dhnOWK2+BkyVZ2lqrPGjh27g9VwOBwDZUeFwG3YJ8ag9FNjtwEfiGYJDgE2FQ0bHA7HCGSri4qIyHXAW4F2EVmGfXHoW8ANIvJhYAlwZhT9TuAkYAHQDXxoCOrscDgGka0KAVV9bz+HjqkSV4GPDbRSDodj5+E8Bh2OGscJAYejxnFCwOGocZwQcDhqHCcEHI4axwkBh6PGcULA4ahxnBBwOGocJwQcjhrHCQGHo8ZxQsDhqHGcEHA4ahwnBByOGscJAYejxnFCwOGocZwQcDhqHCcEHI4axwkBh6PG2dHPkH1HROZHnxr7k4iMKjr2xegzZC+IyPFDVG+HwzFI7OhnyO4F9lHVNwAvAl8EEJG9gbOA10dpfioi/qDV1uFwDDo79BkyVb1HVXPR7qPY9wXAPkN2var2qeoibNXhgwexvg6HY5AZDJvAecBfot/uM2QOx2uMAQkBEfkykAN+vwNp3WfIHI4RwA4LARH5IPB24H3R9wbAfYbM4XjNsUNCQEROAC4C3qmq3UWHbgPOEpE6EZkJ7A48PvBqOhyOoWJHP0P2RaAOuFdEAB5V1f9U1edE5AbgeWyY8DFVDYaq8g6HY+BIQZMfPmbNmqWzZ88e7mo4HP/WiMgcVZ1VHu48Bh2OGscJAYejxnFCwOGocZwQcDhqHCcEHI4axwkBh6PGcULA4ahxnBBwOGocJwQcjhrHCQGHo8ZxQsDhqHGcEHA4ahwnBByOGscJAYejxnFCwOGocZwQcDhqHCcEHI4axwkBh6PG2aHPkBUd+6yIqIi0R/siIldGnyF7RkQOGIpKOxyOwWNHP0OGiEwF3gYsLQo+EVtheHfgAuBnA6+iw+EYSnboM2QR38eWHS9eqfQU4Fo1HgVGicjEQampw+EYEnb0uwOnAMtV9emyQ+4zZA7Ha4ytfnegHBFpAL6EDQV2GBG5ABsyMG3atIFk5XA4BsCOaAK7AjOBp0VkMfapsbkiMgH3GTKH4zXHdgsBVZ2nquNUdYaqzsBU/gNUdSX2GbIPRLMEhwCbVHXF4FbZ4XAMJtsyRXgd8Aiwp4gsE5EPbyH6ncBCYAFwNfD/BqWWDodjyNiqTUBV37uV4zOKfivwsYFXy+Fw7Cycx6DDUeM4IeBw1DhOCDgcNY4TAg5HjeOEgMNR4zgh4HDUOE4IOBw1jhMCDkeN44SAw1HjOCHgcNQ4Tgg4HDWOEwIOR43jhIDDUeM4IeBw1DhOCDgcNY4TAg5HjeOEgMNR4zgh4HDUOE4IOBw1jhMCDkeN44SAw1HjOCHgcNQ4YquED3MlRNYAXcDa4a4L0I6rRzGuHqW8lusxXVUrPvc1IoQAgIjMVtVZrh6uHq4eO7cebjjgcNQ4Tgg4HDXOSBICVw13BSJcPUpx9Sjl364eI8Ym4HA4hoeRpAk4HI5hYNiFgIicICIviMgCEbl4J5Y7VUQeEJHnReQ5EflkFP5VEVkuIk9F20k7oS6LRWReVN7sKGy0iNwrIi9Ff9uGuA57Fp3zUyKyWUQ+tTOuh4j8UkRWi8izRWFVz1+MK6Pn5RkROWCI6/EdEZkflfUnERkVhc8QkZ6i6/LzIa5Hv/dBRL4YXY8XROT47S5QVYdtA3zgZWAXIAU8Dey9k8qeCBwQ/W4GXgT2Br4KfG4nX4fFQHtZ2LeBi6PfFwOX7+T7shKYvjOuB3AkcADw7NbOHzgJ+AsgwCHAY0Ncj7cBiej35UX1mFEcbydcj6r3IXpmnwbqgJlRe/K3p7zh1gQOBhao6kJVzQDXA6fsjIJVdYWqzo1+dwD/AibvjLK3kVOA30S/fwOcuhPLPgZ4WVWX7IzCVPUhYH1ZcH/nfwpwrRqPAqNEZOJQ1UNV71HVXLT7KDBlMMra3npsgVOA61W1T1UXAQuwdrXNDLcQmAy8UrS/jGFoiCIyA9gfeCwKujBS/3451Gp4hAL3iMgcEbkgChuvqiui3yuB8TuhHjFnAdcV7e/s6wH9n/9wPjPnYVpIzEwReVJE/iYiR+yE8qvdhwFfj+EWAsOOiDQBNwGfUtXNwM+AXYH9gBXA93ZCNQ5X1QOAE4GPiciRxQfV9L6dMo0jIingncD/RUHDcT1K2Jnn3x8i8mUgB/w+CloBTFPV/YHPAH8QkZYhrMKQ3YfhFgLLgalF+1OisJ2CiCQxAfB7Vb0ZQFVXqWqgqiFwNdupWu0Iqro8+rsa+FNU5qpYzY3+rh7qekScCMxV1VVRnXb69Yjo7/x3+jMjIh8E3g68LxJIROr3uuj3HGwsvsdQ1WEL92HA12O4hcATwO4iMjPqgc4CbtsZBYuIANcA/1LVK4rCi8eXpwHPlqcd5Ho0ikhz/BszRD2LXYdzo2jnArcOZT2KeC9FQ4GdfT2K6O/8bwM+EM0SHAJsKho2DDoicgJwEfBOVe0uCh8rIn70exdgd2DhENajv/twG3CWiNSJyMyoHo9vV+ZDYd3cTkvoSZhl/mXgyzux3MMxFfMZ4KloOwn4LTAvCr8NmDjE9dgFs+4+DTwXXwNgDHAf8BLwV2D0TrgmjcA6oLUobMivByZ0VgBZbEz74f7OH5sV+En0vMwDZg1xPRZgY+74Gfl5FPf06H49BcwF3jHE9ej3PgBfjq7HC8CJ21ue8xh0OGqc4R4OOByOYcYJAYejxnFCwOGocZwQcDhqHCcEHI4axwkBh6PGcULA4ahxnBBwOGqc/w9AgWfpCQRQFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "\n",
    "#print(len(dataset))\n",
    "\n",
    "data = dataset[14500]\n",
    "#data = dataset[9400]\n",
    "\n",
    "#im = data[\"image\"].transpose(1, 2, 0)\n",
    "im = data[\"image\"].transpose(1,2,0)\n",
    "im = dataset.rasterizer.to_rgb(im)\n",
    "\n",
    "#target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "#draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\n",
    "# how to solve the \"raster from agent\" prediction?\n",
    "\n",
    "#history_positions_pixels = transform_points(data[\"history_positions\"], data[\"raster_from_agent\"])\n",
    "#print(data[\"history_positions\"])\n",
    "#draw_trajectory(im, data[\"negative_positions_pixels\"], TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\n",
    "\n",
    "#print(data[\"target_positions\"])\n",
    "\n",
    "\n",
    "#print(data[\"target_positions\"])\n",
    "target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\n",
    "\"\"\"\n",
    "# build the trajectory sampling function\n",
    "def sample_traj(history_traj, shooting_delta):\n",
    "    res_traj = history_traj.copy()\n",
    "    delta = -0.05\n",
    "    for i in range(len(history_traj)):\n",
    "        delta = delta + shooting_delta\n",
    "        res_traj[i][1] -= delta\n",
    "    return res_traj\n",
    "\n",
    "# we can adjust the shooting_delta to query different sample trajectories\n",
    "sampled_positions_maps = sample_traj(data[\"target_positions\"], shooting_delta = 0.3)\n",
    "\n",
    "#print(sampled_positions_maps)\n",
    "sampled_positions_pixels = transform_points(sampled_positions_maps, data[\"raster_from_agent\"])\n",
    "\n",
    "draw_trajectory(im, sampled_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\"\"\"\n",
    "\n",
    "centerline_area = data[\"goal_list\"]\n",
    "import cv2\n",
    "for k in range(data[\"goal_num\"]):\n",
    "#        print(j)\n",
    "    xy_points = centerline_area[k]\n",
    "#    xy_points = (transform_point(k, raster_from_world))\n",
    "#        print(xy_points)\n",
    "    cv2.circle(im, (int(xy_points[0]),int(xy_points[1])), radius=1, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "gt_goal = data[\"goal_list\"][data[\"goal_gt\"]]\n",
    "#cv2.circle(im, (gt_goal[0],gt_goal[1]), radius = 1, color = (0,0,255), thickness=-1)\n",
    "    \n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "#print(np.round(sampled_positions_pixels,0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                      | 206/33437057 [00:30<1383:34:08,  6.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m goal_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataset)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal_num\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m goal_num:\n\u001b[1;32m      5\u001b[0m         goal_num \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoal_num\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/testenv/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/haolan/l5kit/l5kit/l5kit/dataset/ego.py:265\u001b[0m, in \u001b[0;36mEgoDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    261\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_frame(scene_index, state_index)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# getting our customized data in get_additional\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# data = self.get_additional_info_grid_goal(data)\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_additional_info_centerline_goal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# print(\"data time: {}\".format(time.time()-start))\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/haolan/l5kit/l5kit/l5kit/dataset/ego.py:286\u001b[0m, in \u001b[0;36mEgoDataset.get_additional_info_centerline_goal\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    283\u001b[0m current_positions_pixels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m27\u001b[39m,\u001b[38;5;241m56\u001b[39m]  \u001b[38;5;66;03m# a bit backward to include the AV stands still\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# TODO, raster_radius/4 to adjust target numbers\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m lane_indices \u001b[38;5;241m=\u001b[39m \u001b[43mindices_in_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenter_in_world\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msem_rast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapAPI\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbounds_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlanes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbounds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraster_radius\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01ml5kit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmap_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpolationMethod, MapAPI, TLFacesColors\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n",
      "File \u001b[0;32m~/haolan/l5kit/l5kit/l5kit/rasterization/semantic_rasterizer.py:52\u001b[0m, in \u001b[0;36mindices_in_bounds\u001b[0;34m(center, bounds, half_extent)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03mGet indices of elements for which the bounding box described by bounds intersects the one defined around\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03mcenter (square with side 2*half_side)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    np.ndarray: indices of elements inside radius from center\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m x_center, y_center \u001b[38;5;241m=\u001b[39m center\n\u001b[0;32m---> 52\u001b[0m x_min_in \u001b[38;5;241m=\u001b[39m \u001b[43mx_center\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhalf_extent\u001b[49m\n\u001b[1;32m     53\u001b[0m y_min_in \u001b[38;5;241m=\u001b[39m y_center \u001b[38;5;241m>\u001b[39m bounds[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m half_extent\n\u001b[1;32m     54\u001b[0m x_max_in \u001b[38;5;241m=\u001b[39m x_center \u001b[38;5;241m<\u001b[39m bounds[:, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m half_extent\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "goal_num = -1\n",
    "import tqdm\n",
    "for step, data in enumerate(tqdm.tqdm(dataset)):\n",
    "    if data[\"goal_num\"] > goal_num:\n",
    "        goal_num = data[\"goal_num\"]\n",
    "print(goal_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n"
     ]
    }
   ],
   "source": [
    "print(goal_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gt_goal_positions_pixels = transform_point((data[\"target_positions\"][-1,:2]), data[\"raster_from_agent\"])\n",
    "#print(gt_goal_positions_pixels)\n",
    "#xy = (data[\"goal_list\"]-gt_goal_positions_pixels)\n",
    "#xyz\n",
    "\n",
    "#print(np.argmax(np.linalg.norm(xy, axis=-1)))\n",
    "#print(data[\"goal_gt\"])\n",
    "import torch\n",
    "from torch import nn\n",
    "a = torch.rand(64,500)\n",
    "b = torch.ones(64, dtype=torch.long)\n",
    "\n",
    "loss = nn.NLLLoss(reduction='none')(a,b)[:4]\n",
    "#print(loss.shape)\n",
    "\n",
    "print(data[\"goal_list\"][data[\"goal_gt\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerendering the semantic map in the image space\n",
    "\n",
    "import cv2\n",
    "from l5kit.rasterization.semantic_rasterizer import indices_in_bounds\n",
    "from l5kit.geometry import rotation33_as_yaw, transform_point, transform_points\n",
    "from l5kit.data.map_api import InterpolationMethod, MapAPI, TLFacesColors\n",
    "\n",
    "from collections import defaultdict\n",
    "from enum import IntEnum\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "img = 255 * np.ones(shape=(rast.raster_size[1], rast.raster_size[0], 3), dtype=np.uint8)\n",
    "\n",
    "data = dataset[515]\n",
    "\n",
    "\n",
    "\n",
    "current_positions_pixels = transform_points(data[\"history_positions\"], data[\"raster_from_agent\"])[-1]\n",
    "\n",
    "#cv2.circle(img, (int(current_positions_pixels[0]),int(current_positions_pixels[1])), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "\n",
    "raster_from_world = data[\"raster_from_world\"]\n",
    "world_from_raster = np.linalg.inv(raster_from_world)\n",
    "#print(raster_from_world)\n",
    "\n",
    "raster_radius = float(np.linalg.norm(rast.raster_size * rast.pixel_size)) / 4\n",
    "#raster_radius = 0\n",
    "\n",
    "center_in_raster_px = np.asarray(rast.raster_size) * (0.25, 0.5)\n",
    "center_in_world = transform_point(center_in_raster_px, world_from_raster)\n",
    "#print(center_in_world)\n",
    "lane_indices = indices_in_bounds(center_in_world, rast.sem_rast.mapAPI.bounds_info[\"lanes\"][\"bounds\"], raster_radius)\n",
    "\n",
    "#lane_indices = lane_indices[0:1]\n",
    "print(lane_indices)\n",
    "\n",
    "\n",
    "CV2_SUB_VALUES = {\"shift\": 9, \"lineType\": cv2.LINE_AA}\n",
    "CV2_SHIFT_VALUE = 2 ** CV2_SUB_VALUES[\"shift\"]\n",
    "\n",
    "\n",
    "\n",
    "class RasterEls(IntEnum):  # map elements\n",
    "    LANE_NOTL = 0\n",
    "    ROAD = 1\n",
    "    CROSSWALK = 2\n",
    "    \n",
    "COLORS = {\n",
    "    TLFacesColors.GREEN.name: (0, 255, 0),\n",
    "    TLFacesColors.RED.name: (255, 0, 0),\n",
    "    TLFacesColors.YELLOW.name: (255, 255, 0),\n",
    "    RasterEls.LANE_NOTL.name: (255, 217, 82),\n",
    "    RasterEls.ROAD.name: (17, 17, 31),\n",
    "    RasterEls.CROSSWALK.name: (255, 117, 69),\n",
    "}\n",
    "    \n",
    "def cv2_subpixel(coords: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cast coordinates to numpy.int but keep fractional part by previously multiplying by 2**CV2_SHIFT\n",
    "    cv2 calls will use shift to restore original values with higher precision\n",
    "    Args:\n",
    "        coords (np.ndarray): XY coords as float\n",
    "    Returns:\n",
    "        np.ndarray: XY coords as int for cv2 shift draw\n",
    "    \"\"\"\n",
    "    coords = coords * CV2_SHIFT_VALUE\n",
    "    coords = coords.astype(np.int)\n",
    "    return coords    \n",
    "\n",
    "# TB Tested\n",
    "INTERPOLATION_POINTS = 20\n",
    "lanes_mask: Dict[str, np.ndarray] = defaultdict(lambda: np.zeros(len(lane_indices) * 2, dtype=np.bool))\n",
    "lanes_area = np.zeros((len(lane_indices) * 2, INTERPOLATION_POINTS, 2))\n",
    "\n",
    "\n",
    "self = rast.sem_rast\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for idx, lane_idx in enumerate(lane_indices):\n",
    "    lane_idx = self.mapAPI.bounds_info[\"lanes\"][\"ids\"][lane_idx]\n",
    "\n",
    "    lane_dict = self.mapAPI.get_lane_coords(lane_idx)\n",
    "    xyz_left = lane_dict[\"xyz_left\"][:,:2]\n",
    "    xyz_right = lane_dict[\"xyz_right\"][:,:2]\n",
    "#    print(xyz_left.shape)\n",
    " \n",
    "    lane_coords = self.mapAPI.get_lane_as_interpolation(\n",
    "        lane_idx, INTERPOLATION_POINTS, InterpolationMethod.INTER_ENSURE_LEN\n",
    "    )\n",
    "    # interpolate over polyline to always have the same number of points\n",
    "#    print(lane_coords[\"xyz_left\"].shape)\n",
    "    lanes_area[idx * 2] = lane_coords[\"xyz_left\"][:, :2]\n",
    "    lanes_area[idx * 2 + 1] = lane_coords[\"xyz_right\"][::-1, :2]\n",
    "\n",
    "    lane_type = RasterEls.LANE_NOTL.name\n",
    "#    lane_tl_ids = set(self.mapAPI.get_lane_traffic_control_ids(lane_idx))\n",
    "#    for tl_id in lane_tl_ids.intersection(active_tl_ids):\n",
    "#        lane_type = self.mapAPI.get_color_for_face(tl_id)\n",
    "\n",
    "    lanes_mask[lane_type][idx * 2: idx * 2 + 2] = True\n",
    "\n",
    "    \n",
    "\n",
    "def goalGen(map_api, lane_indices, current_pixels, raster_from_world):\n",
    "    centerline_area = []\n",
    "    \n",
    "    for idx, lane_idx in enumerate(lane_indices):\n",
    "        lane_idx = map_api.bounds_info[\"lanes\"][\"ids\"][lane_idx]\n",
    "\n",
    "        lane_dict = map_api.get_lane_coords(lane_idx)\n",
    "\n",
    "        xyz_left = lane_dict[\"xyz_left\"]\n",
    "        xyz_right = lane_dict[\"xyz_right\"]\n",
    "\n",
    "        mid_steps = 3\n",
    "        xyz_left = self.mapAPI.interpolate(xyz_left, mid_steps, InterpolationMethod.INTER_METER)\n",
    "        xyz_right = self.mapAPI.interpolate(xyz_right, mid_steps, InterpolationMethod.INTER_METER)\n",
    "        if (xyz_left).shape[0] < (xyz_right).shape[0]:\n",
    "            xyz_right = xyz_right[:(xyz_left).shape[0]]\n",
    "\n",
    "        if (xyz_left).shape[0] > (xyz_right).shape[0]:\n",
    "            xyz_left = xyz_left[:(xyz_right).shape[0]]\n",
    "\n",
    "        xyz_center = (xyz_left+xyz_right)/2\n",
    "        xyz_center = xyz_center[:,:2]\n",
    "        # get_lane_as_interpolation is stateful function\n",
    "        # use stateless interpolate instead\n",
    "\n",
    "        # if the distance is bigger than TARGET_THRESHOLD\n",
    "        from math import sqrt\n",
    "        TARGET_THRESHOLD = 3\n",
    "\n",
    "        for p in xyz_center:\n",
    "            valid = True\n",
    "            for q in centerline_area:\n",
    "                d = q-p\n",
    "                dist = sqrt((d[0]*d[0]+d[1]*d[1]))\n",
    "                if dist < TARGET_THRESHOLD:\n",
    "                    valid = False\n",
    "\n",
    "            if valid:\n",
    "                centerline_area.append(p)   \n",
    "\n",
    "\n",
    "    print(len(centerline_area))\n",
    "    tmp_area = centerline_area\n",
    "    centerline_area = []\n",
    "\n",
    "    # traverse the centerline_area to filter\n",
    "    for k in tmp_area:\n",
    "        xy_points = (transform_point(k, raster_from_world))\n",
    "        if current_pixels[0] < xy_points[0] :\n",
    "            centerline_area.append(k)\n",
    "    \n",
    "    print(len(centerline_area))\n",
    "    return centerline_area\n",
    "\n",
    "centerline_area = goalGen(self.mapAPI, lane_indices, current_positions_pixels, raster_from_world)\n",
    "\n",
    "\n",
    "\n",
    "# reserve 200 points? \n",
    "\n",
    "   \n",
    "if len(lanes_area):\n",
    "    lanes_area = cv2_subpixel(transform_points(lanes_area.reshape((-1, 2)), raster_from_world))\n",
    "\n",
    "#    for lane_area in lanes_area.reshape((-1, INTERPOLATION_POINTS * 2, 2)):\n",
    "        # need to for-loop otherwise some of them are empty\n",
    "#        cv2.fillPoly(img, [lane_area], COLORS[RasterEls.ROAD.name], **CV2_SUB_VALUES)\n",
    "\n",
    "    lanes_area = lanes_area.reshape((-1, INTERPOLATION_POINTS, 2))\n",
    "    for name, mask in lanes_mask.items():  # draw each type of lane with its own color\n",
    "        cv2.polylines(img, lanes_area[mask], False, COLORS[name], **CV2_SUB_VALUES)\n",
    "\n",
    "#xy_points = (data[\"goal_list\"][data[\"goal_gt\"]])\n",
    "#        print(xy_points)\n",
    "#cv2.circle(img, (int(xy_points[0]),int(xy_points[1])), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "# can we pick the closest ten points?\n",
    "\n",
    "\n",
    "for k in centerline_area:\n",
    "#        print(j)\n",
    "    xy_points = (transform_point(k, raster_from_world))\n",
    "#        print(xy_points)\n",
    "    cv2.circle(img, (int(xy_points[0]),int(xy_points[1])), radius=1, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_in_bounds(center: np.ndarray, bounds: np.ndarray, half_extent: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get indices of elements for which the bounding box described by bounds intersects the one defined around\n",
    "    center (square with side 2*half_side)\n",
    "    Args:\n",
    "        center (float): XY of the center\n",
    "        bounds (np.ndarray): array of shape Nx2x2 [[x_min,y_min],[x_max, y_max]]\n",
    "        half_extent (float): half the side of the bounding box centered around center\n",
    "    Returns:\n",
    "        np.ndarray: indices of elements inside radius from center\n",
    "    \"\"\"\n",
    "    x_center, y_center = center\n",
    "\n",
    "    x_min_in = x_center > bounds[:, 0, 0] - half_extent\n",
    "    y_min_in = y_center > bounds[:, 0, 1] - half_extent\n",
    "    x_max_in = x_center < bounds[:, 1, 0] + half_extent\n",
    "    y_max_in = y_center < bounds[:, 1, 1] + half_extent\n",
    "    return np.nonzero(x_min_in & y_min_in & x_max_in & y_max_in)[0]\n",
    "\n",
    "\n",
    "CV2_SUB_VALUES = {\"shift\": 9, \"lineType\": cv2.LINE_AA}\n",
    "CV2_SHIFT_VALUE = 2 ** CV2_SUB_VALUES[\"shift\"]\n",
    "\n",
    "\n",
    "\n",
    "class RasterEls(IntEnum):  # map elements\n",
    "    LANE_NOTL = 0\n",
    "    ROAD = 1\n",
    "    CROSSWALK = 2\n",
    "    \n",
    "    \n",
    "    \n",
    "COLORS = {\n",
    "    TLFacesColors.GREEN.name: (0, 255, 0),\n",
    "    TLFacesColors.RED.name: (255, 0, 0),\n",
    "    TLFacesColors.YELLOW.name: (255, 255, 0),\n",
    "    RasterEls.LANE_NOTL.name: (255, 217, 82),\n",
    "    RasterEls.ROAD.name: (17, 17, 31),\n",
    "    RasterEls.CROSSWALK.name: (255, 117, 69),\n",
    "}\n",
    "    \n",
    "def cv2_subpixel(coords: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cast coordinates to numpy.int but keep fractional part by previously multiplying by 2**CV2_SHIFT\n",
    "    cv2 calls will use shift to restore original values with higher precision\n",
    "    Args:\n",
    "        coords (np.ndarray): XY coords as float\n",
    "    Returns:\n",
    "        np.ndarray: XY coords as int for cv2 shift draw\n",
    "    \"\"\"\n",
    "    coords = coords * CV2_SHIFT_VALUE\n",
    "    coords = coords.astype(np.int)\n",
    "    return coords    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerendering the semantic map in the image space\n",
    "\n",
    "import cv2\n",
    "from l5kit.geometry import rotation33_as_yaw, transform_point, transform_points\n",
    "from l5kit.data.map_api import InterpolationMethod, MapAPI, TLFacesColors\n",
    "\n",
    "from collections import defaultdict\n",
    "from enum import IntEnum\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "img = 255 * np.ones(shape=(rast.raster_size[1], rast.raster_size[0], 3), dtype=np.uint8)\n",
    "\n",
    "data = dataset[9400]\n",
    "#data = dataset[132060]\n",
    "\n",
    "current_positions_pixels = transform_points(data[\"history_positions\"], data[\"raster_from_agent\"])[-1]\n",
    "\n",
    "\n",
    "raster_from_world = data[\"raster_from_world\"]\n",
    "world_from_raster = np.linalg.inv(raster_from_world)\n",
    "#print(raster_from_world)\n",
    "\n",
    "raster_radius = float(np.linalg.norm(rast.raster_size * rast.pixel_size)) / 2\n",
    "#raster_radius = 0\n",
    "\n",
    "#center_in_raster_px = np.asarray(rast.raster_size) * (0.25, 0.5)\n",
    "center_in_raster_px = np.asarray(rast.raster_size) * (0.5, 0.5)\n",
    "\n",
    "center_in_world = transform_point(center_in_raster_px, world_from_raster)\n",
    "\n",
    "#ego_in_world = transform_point(current_positions_pixels, world_from_raster)\n",
    "\n",
    "# convert bound coordinates (world space) to raster space\n",
    "#lane_bound_world = rast.sem_rast.mapAPI.bounds_info[\"lanes\"][\"bounds\"]\n",
    "#lane_bound_raster = lane_bound_world\n",
    "\n",
    "#for i in range(lane_bound_world.shape[0]):\n",
    "#    lane_bound_world[i, 0] = transform_point(lane_bound_world[i, 0], raster_from_world)\n",
    "#    lane_bound_world[i, 1] = transform_point(lane_bound_world[i, 1], raster_from_world)\n",
    "\n",
    "\n",
    "#print(center_in_world)\n",
    "\n",
    "#print(center_in_world)\n",
    "\n",
    "# judge in the rasterized space will be good\n",
    "\n",
    "\n",
    "\n",
    "# compute the indices in the raster space\n",
    "\n",
    "\n",
    "lane_indices = indices_in_bounds(center_in_world, rast.sem_rast.mapAPI.bounds_info[\"lanes\"][\"bounds\"], raster_radius)\n",
    "\n",
    "\n",
    "# TB Tested\n",
    "INTERPOLATION_POINTS = 20\n",
    "lanes_mask: Dict[str, np.ndarray] = defaultdict(lambda: np.zeros(len(lane_indices) * 2, dtype=np.bool))\n",
    "lanes_area = np.zeros((len(lane_indices) * 2, INTERPOLATION_POINTS, 2))\n",
    "\n",
    "\n",
    "self = rast.sem_rast\n",
    "\n",
    "centerline_area = []\n",
    "\n",
    "\n",
    "\n",
    "for idx, lane_idx in enumerate(lane_indices):\n",
    "    lane_idx = self.mapAPI.bounds_info[\"lanes\"][\"ids\"][lane_idx]\n",
    "\n",
    "    \n",
    "    lane_dict = self.mapAPI.get_lane_coords(lane_idx)\n",
    "    \n",
    "    \n",
    "    xyz_left = lane_dict[\"xyz_left\"][:,:2]\n",
    "    xyz_right = lane_dict[\"xyz_right\"][:,:2]\n",
    "#    print(xyz_left.shape)\n",
    " \n",
    "    lane_coords = self.mapAPI.get_lane_as_interpolation(\n",
    "        lane_idx, INTERPOLATION_POINTS, InterpolationMethod.INTER_ENSURE_LEN\n",
    "    )\n",
    "    # interpolate over polyline to always have the same number of points\n",
    "#    print(lane_coords[\"xyz_left\"].shape)\n",
    "    lanes_area[idx * 2] = lane_coords[\"xyz_left\"][:, :2]\n",
    "    lanes_area[idx * 2 + 1] = lane_coords[\"xyz_right\"][::-1, :2]\n",
    "\n",
    "    lane_type = RasterEls.LANE_NOTL.name\n",
    "    \n",
    "\n",
    "    lanes_mask[lane_type][idx * 2: idx * 2 + 2] = True\n",
    "\n",
    "\n",
    "\n",
    "for idx, lane_idx in enumerate(lane_indices):\n",
    "    lane_idx = self.mapAPI.bounds_info[\"lanes\"][\"ids\"][lane_idx]\n",
    "    \n",
    "    lane_dict = self.mapAPI.get_lane_coords(lane_idx)\n",
    "    \n",
    "    xyz_left = lane_dict[\"xyz_left\"]\n",
    "    xyz_right = lane_dict[\"xyz_right\"]\n",
    "    \n",
    "    mid_steps = 2\n",
    "    xyz_left = self.mapAPI.interpolate(xyz_left, mid_steps, InterpolationMethod.INTER_METER)\n",
    "    xyz_right = self.mapAPI.interpolate(xyz_right, mid_steps, InterpolationMethod.INTER_METER)\n",
    "    if (xyz_left).shape[0] < (xyz_right).shape[0]:\n",
    "        xyz_right = xyz_right[:(xyz_left).shape[0]]\n",
    "     \n",
    "    if (xyz_left).shape[0] > (xyz_right).shape[0]:\n",
    "        xyz_left = xyz_left[:(xyz_right).shape[0]]\n",
    "    \n",
    "    xyz_center = (xyz_left+xyz_right)/2\n",
    "    \n",
    "    xyz_center = xyz_center[:,:2]\n",
    "\n",
    "    from math import sqrt\n",
    "    TARGET_THRESHOLD = 2\n",
    "    \n",
    "    for p in xyz_center:\n",
    "        valid = True\n",
    "        for q in centerline_area:\n",
    "            d = q-p\n",
    "            dist = sqrt((d[0]*d[0]+d[1]*d[1]))\n",
    "            if dist < TARGET_THRESHOLD:\n",
    "                valid = False\n",
    "\n",
    "        if valid:\n",
    "            centerline_area.append(p)   \n",
    "        \n",
    "\n",
    "        \n",
    "tmp_area = centerline_area\n",
    "centerline_area = []\n",
    "# traverse the centerline_area to filter\n",
    "for k in tmp_area:\n",
    "#        print(j)\n",
    "        xy_points = (transform_point(k, raster_from_world))\n",
    "        if current_positions_pixels[0] < xy_points[0] :\n",
    "            centerline_area.append(k)\n",
    "#        print(xy_points)\n",
    "\n",
    "\n",
    "print(len(centerline_area))\n",
    "# reserve 200 points? \n",
    "\n",
    "   \n",
    "if len(lanes_area):\n",
    "    lanes_area = cv2_subpixel(transform_points(lanes_area.reshape((-1, 2)), raster_from_world))\n",
    "\n",
    "    for lane_area in lanes_area.reshape((-1, INTERPOLATION_POINTS * 2, 2)):\n",
    "        # need to for-loop otherwise some of them are empty\n",
    "        cv2.fillPoly(img, [lane_area], COLORS[RasterEls.ROAD.name], **CV2_SUB_VALUES)\n",
    "\n",
    "    lanes_area = lanes_area.reshape((-1, INTERPOLATION_POINTS, 2))\n",
    "    for name, mask in lanes_mask.items():  # draw each type of lane with its own color\n",
    "        cv2.polylines(img, lanes_area[mask], False, COLORS[name], **CV2_SUB_VALUES)\n",
    "\n",
    "#print(lane_area)\n",
    "        \n",
    "        \n",
    "#xy_points = (data[\"goal_list\"][data[\"goal_gt\"]])\n",
    "#        print(xy_points)\n",
    "#cv2.circle(img, (int(xy_points[0]),int(xy_points[1])), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "for k in centerline_area:\n",
    "#        print(j)\n",
    "        xy_points = (transform_point(k, raster_from_world))\n",
    "#        print(xy_points)\n",
    "        cv2.circle(img, (int(xy_points[0]),int(xy_points[1])), radius=1, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "#print(current_positions_pixels[0])\n",
    "#cv2.circle(img, (round(current_positions_pixels[0]) , round(current_positions_pixels[1])) ,radius=1, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerendering the semantic map in the image space\n",
    "\n",
    "import cv2\n",
    "from l5kit.geometry import rotation33_as_yaw, transform_point, transform_points\n",
    "from l5kit.data.map_api import InterpolationMethod, MapAPI, TLFacesColors\n",
    "\n",
    "from collections import defaultdict\n",
    "from enum import IntEnum\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "img = 255 * np.ones(shape=(rast.raster_size[1], rast.raster_size[0], 3), dtype=np.uint8)\n",
    "\n",
    "data = dataset[14500]\n",
    "#data = dataset[1200]\n",
    "#data = dataset[2060]\n",
    "\n",
    "current_positions_pixels = transform_points(data[\"history_positions\"], data[\"raster_from_agent\"])[-1]\n",
    "\n",
    "\n",
    "raster_from_world = data[\"raster_from_world\"]\n",
    "world_from_raster = np.linalg.inv(raster_from_world)\n",
    "#print(raster_from_world)\n",
    "\n",
    "raster_radius = float(np.linalg.norm(rast.raster_size * rast.pixel_size)) / 2\n",
    "#raster_radius = 0\n",
    "\n",
    "center_in_raster_px = np.asarray(rast.raster_size) * (0.25, 0.5)\n",
    "center_in_world = transform_point(center_in_raster_px, world_from_raster)\n",
    "\n",
    "ego_in_world = transform_point(current_positions_pixels, world_from_raster)\n",
    "\n",
    "# judge in the rasterized space will be good\n",
    "\n",
    "\n",
    "\n",
    "# compute the indices in the raster space\n",
    "\n",
    "\n",
    "lane_indices_pre = indices_in_bounds(ego_in_world, rast.sem_rast.mapAPI.bounds_info[\"lanes\"][\"bounds\"], raster_radius)\n",
    "#lane_indices = indices_in_bounds(current_positions_pixels, lane_bound_raster, 0)\n",
    "\n",
    "\n",
    "#lane_indices = lane_indices[0:1]\n",
    "print((lane_indices_pre))\n",
    "\n",
    "\n",
    "# filter the lane to extract the only lane that the ego vehicle is in.\n",
    "map_api = rast.sem_rast.mapAPI\n",
    "\n",
    "lane_indices = []\n",
    "for lane_id in lane_indices_pre:\n",
    "    \n",
    "    # need to translate to the bounds_info array\n",
    "    lane_id_str = map_api.bounds_info[\"lanes\"][\"ids\"][lane_id]\n",
    "    \n",
    "    # if map_api.is_lane(map_api[lane_id_str]):\n",
    "    lane = map_api.get_lane_coords(lane_id_str)#map_api.id_as_str((map_api[lane_id]).id))\n",
    "    left_lane = lane['xyz_left'][:,:2]\n",
    "    right_lane = lane['xyz_right'][:,:2]\n",
    "\n",
    "    left_lane_rt = transform_points(left_lane, raster_from_world)\n",
    "    right_lane_rt = transform_points(right_lane, raster_from_world) \n",
    "#    lane_indices.append(lane_id)\n",
    "    \n",
    "    x_min = min(np.min(left_lane_rt[:, 0]), np.min(right_lane_rt[:, 0]))\n",
    "    y_min = min(np.min(left_lane_rt[:, 1]), np.min(right_lane_rt[:, 1]))\n",
    "    x_max = max(np.max(left_lane_rt[:, 0]), np.max(right_lane_rt[:, 0]))\n",
    "    y_max = max(np.max(left_lane_rt[:, 1]), np.max(right_lane_rt[:, 1]))\n",
    "\n",
    "    # judge if the lane is \n",
    "    valid = (current_positions_pixels[0] >= x_min) & (current_positions_pixels[0] <= x_max) \\\n",
    "                & (current_positions_pixels[1] >= y_min) & (current_positions_pixels[1] <= y_max)\n",
    "\n",
    "    valid = True\n",
    "    if valid:\n",
    "        lane_indices.append(lane_id_str)\n",
    "    \n",
    "\n",
    "\n",
    "print(lane_indices)\n",
    "\n",
    "\n",
    "# TB Tested\n",
    "INTERPOLATION_POINTS = 20\n",
    "lanes_mask: Dict[str, np.ndarray] = defaultdict(lambda: np.zeros(len(lane_indices) * 2, dtype=np.bool))\n",
    "lanes_area = np.zeros((len(lane_indices) * 2, INTERPOLATION_POINTS, 2))\n",
    "\n",
    "\n",
    "self = rast.sem_rast\n",
    "\n",
    "centerline_area = []\n",
    "\n",
    "\n",
    "\n",
    "for idx, lane_idx in enumerate(lane_indices):\n",
    "#    lane_idx = self.mapAPI.bounds_info[\"lanes\"][\"ids\"][lane_idx]\n",
    "\n",
    "    \n",
    "    lane_dict = self.mapAPI.get_lane_coords(lane_idx)\n",
    "    \n",
    "    \n",
    "    xyz_left = lane_dict[\"xyz_left\"][:,:2]\n",
    "    xyz_right = lane_dict[\"xyz_right\"][:,:2]\n",
    "#    print(xyz_left.shape)\n",
    " \n",
    "    lane_coords = self.mapAPI.get_lane_as_interpolation(\n",
    "        lane_idx, INTERPOLATION_POINTS, InterpolationMethod.INTER_ENSURE_LEN\n",
    "    )\n",
    "    # interpolate over polyline to always have the same number of points\n",
    "#    print(lane_coords[\"xyz_left\"].shape)\n",
    "    lanes_area[idx * 2] = lane_coords[\"xyz_left\"][:, :2]\n",
    "    lanes_area[idx * 2 + 1] = lane_coords[\"xyz_right\"][::-1, :2]\n",
    "\n",
    "    lane_type = RasterEls.LANE_NOTL.name\n",
    "    \n",
    "#    print(lane_type)\n",
    "    \n",
    "#    lane_tl_ids = set(self.mapAPI.get_lane_traffic_control_ids(lane_idx))\n",
    "#    for tl_id in lane_tl_ids.intersection(active_tl_ids):\n",
    "#        lane_type = self.mapAPI.get_color_for_face(tl_id)\n",
    "\n",
    "    lanes_mask[lane_type][idx * 2: idx * 2 + 2] = True\n",
    "\n",
    "\n",
    "\n",
    "for idx, lane_idx in enumerate(lane_indices):\n",
    "#    lane_idx = self.mapAPI.bounds_info[\"lanes\"][\"ids\"][lane_idx]\n",
    "    \n",
    "    lane_dict = self.mapAPI.get_lane_coords(lane_idx)\n",
    "    \n",
    "    xyz_left = lane_dict[\"xyz_left\"]\n",
    "    xyz_right = lane_dict[\"xyz_right\"]\n",
    "    \n",
    "    mid_steps = 5\n",
    "    xyz_left = self.mapAPI.interpolate(xyz_left, mid_steps, InterpolationMethod.INTER_METER)\n",
    "    xyz_right = self.mapAPI.interpolate(xyz_right, mid_steps, InterpolationMethod.INTER_METER)\n",
    "    if (xyz_left).shape[0] < (xyz_right).shape[0]:\n",
    "        xyz_right = xyz_right[:(xyz_left).shape[0]]\n",
    "     \n",
    "    if (xyz_left).shape[0] > (xyz_right).shape[0]:\n",
    "        xyz_left = xyz_left[:(xyz_right).shape[0]]\n",
    "    \n",
    "    xyz_center = (xyz_left+xyz_right)/2\n",
    "    \n",
    "    \n",
    "#    xyz_center = self.mapAPI.interpolate(xyz_center, mid_steps, InterpolationMethod.INTER_METER)\n",
    "    xyz_center = xyz_center[:,:2]\n",
    "    # get_lane_as_interpolation is stateful function\n",
    "    # use stateless interpolate instead\n",
    "    \n",
    "    # if the distance is bigger than TARGET_THRESHOLD\n",
    "    from math import sqrt\n",
    "    TARGET_THRESHOLD = 2\n",
    "    \n",
    "    for p in xyz_center:\n",
    "        valid = True\n",
    "        for q in centerline_area:\n",
    "            d = q-p\n",
    "            dist = sqrt((d[0]*d[0]+d[1]*d[1]))\n",
    "            if dist < TARGET_THRESHOLD:\n",
    "                valid = False\n",
    "\n",
    "        if valid:\n",
    "            centerline_area.append(p)   \n",
    "        \n",
    "        \n",
    "tmp_area = centerline_area\n",
    "centerline_area = []\n",
    "# traverse the centerline_area to filter\n",
    "for k in tmp_area:\n",
    "#        print(j)\n",
    "        xy_points = (transform_point(k, raster_from_world))\n",
    "        if current_positions_pixels[0] < xy_points[0] :\n",
    "            centerline_area.append(k)\n",
    "#        print(xy_points)\n",
    "\n",
    "\n",
    "\n",
    "# reserve 200 points? \n",
    "\n",
    "   \n",
    "if len(lanes_area):\n",
    "    lanes_area = cv2_subpixel(transform_points(lanes_area.reshape((-1, 2)), raster_from_world))\n",
    "\n",
    "    for lane_area in lanes_area.reshape((-1, INTERPOLATION_POINTS * 2, 2)):\n",
    "        # need to for-loop otherwise some of them are empty\n",
    "        cv2.fillPoly(img, [lane_area], COLORS[RasterEls.ROAD.name], **CV2_SUB_VALUES)\n",
    "\n",
    "    lanes_area = lanes_area.reshape((-1, INTERPOLATION_POINTS, 2))\n",
    "    for name, mask in lanes_mask.items():  # draw each type of lane with its own color\n",
    "        cv2.polylines(img, lanes_area[mask], False, COLORS[name], **CV2_SUB_VALUES)\n",
    "\n",
    "#print(lane_area)\n",
    "        \n",
    "        \n",
    "#xy_points = (data[\"goal_list\"][data[\"goal_gt\"]])\n",
    "#        print(xy_points)\n",
    "#cv2.circle(img, (int(xy_points[0]),int(xy_points[1])), radius=3, color=(0, 0, 255), thickness=-1)\n",
    "\n",
    "for k in centerline_area:\n",
    "#        print(j)\n",
    "        xy_points = (transform_point(k, raster_from_world))\n",
    "#        print(xy_points)\n",
    "        cv2.circle(img, (int(xy_points[0]),int(xy_points[1])), radius=1, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "#print(current_positions_pixels[0])\n",
    "#cv2.circle(img, (round(current_positions_pixels[0]) , round(current_positions_pixels[1])) ,radius=1, color=(255, 0, 0), thickness=-1)\n",
    "\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_api = rast.sem_rast.mapAPI\n",
    "\n",
    "# explore the existing lanes\n",
    "# \n",
    "\"\"\"\n",
    "for element in map_api.elements:\n",
    "    element_id = MapAPI.id_as_str(element.id)\n",
    "    print(element_id)\n",
    "    \n",
    "    if map_api.is_lane(element):\n",
    "        lane = map_api.get_lane_coords(element_id)\n",
    "        break\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#lane_str = self.mapAPI.bounds_info[\"lanes\"][\"ids\"][3059]\n",
    "lane_str = 'V6A5'\n",
    "\n",
    "print(map_api[lane_str].element.lane)\n",
    "print(type(map_api[lane_str].element))\n",
    "    \n",
    "#lane = map_api.get_lane_coords(map_api.id_as_str((map_api[1043]).id))\n",
    "#left_lane = lane['xyz_left'][:,:2]\n",
    "#print(ego_in_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for q in rast.sem_rast.mapAPI.bounds_info[\"lanes\"][\"ids\"]:\n",
    "    if not map_api.is_lane(map_api[q]):\n",
    "         cnt = cnt + 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAP_LAYERS = [\"junction\", \"node\", \"segment\", \"lane\"]\n",
    "\n",
    "\n",
    "def element_of_type(elem, layer_name):\n",
    "    return elem.element.HasField(layer_name)\n",
    "\n",
    "\n",
    "def get_elements_from_layer(map_api, layer_name):\n",
    "    return [elem for elem in map_api.elements if element_of_type(elem, layer_name)]\n",
    "\n",
    "all_lanes = get_elements_from_layer(rast.sem_rast.mapAPI, \"lane\")\n",
    "all_lane_dict = {}\n",
    "\n",
    "for lane in all_lanes:\n",
    "    all_lane_dict[rast.sem_rast.mapAPI.id_as_str(lane.id)] = lane\n",
    "\n",
    "    \n",
    "#print(all_lane_dict.keys())\n",
    "#print(all_lane_dict[rast.sem_rast.mapAPI.id_as_str(7098)])\n",
    "#print(all_lanes[10].element.lane.adjacent_lane_change_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation code\n",
    "import cv2\n",
    "centerline_area = data[\"goal_list\"]\n",
    "for k in centerline_area:\n",
    "#        print(j)\n",
    "#        xy_points = (transform_point(k, raster_from_world))\n",
    "#        print(xy_points)\n",
    "    cv2.circle(im, (int(k[0]),int(k[1])), radius=1, color=(255, 0, 0), thickness=-1)\n",
    "print(len(centerline_area))\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "target_matrix=np.zeros((len(data[\"goal_list\"]),2),dtype=int)\n",
    "for k in range(len(data[\"goal_list\"])):\n",
    "#        print(j)\n",
    "#        xy_points = (transform_point(k, raster_from_world))\n",
    "#        print(xy_points)\n",
    "    target_matrix[k,:] = np.array([int(data[\"goal_list\"][k][0]),int(data[\"goal_list\"][k][1])])\n",
    "print(\"time: {} s\".format(time.time()-start))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#print(cfg)\n",
    "train_cfg = cfg[\"val_data_loader\"]\n",
    "print(train_cfg[\"batch_size\"])\n",
    "train_dataloader = DataLoader(dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n",
    "                             num_workers=train_cfg[\"num_workers\"])\n",
    "\n",
    "#target_mlp = MLP(in_channels=(2048+2), out_channels=1, hidden_unit=128)\n",
    "#goal_tensor = torch.tensor(target_matrix)\n",
    "\n",
    "# adding a training test to validate our idea!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = next(iter(train_dataloader))\n",
    "# torch.Size([64, 500, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_data[\"history_positions\"].shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "goal_batch = sample_data[\"goal_list\"]\n",
    "target_mlp = MLP(in_channels=(2048+2), out_channels=1, hidden_unit=128)\n",
    "\n",
    "feature_batch = torch.rand((64,2048))\n",
    "feature_batch=feature_batch.unsqueeze(1).repeat(1,500,1)\n",
    "input_batch = torch.cat([feature_batch,goal_batch],dim=2)\n",
    "\n",
    "x = target_mlp(input_batch).squeeze(dim=2)\n",
    "\n",
    "#mask invalid part\n",
    "for batch_num in range(x.shape[0]):\n",
    "    x[batch_num][sample_data[\"goal_num\"][batch_num]:] = -float(\"inf\")\n",
    "\n",
    "softmax_layer = nn.Softmax(dim=1)\n",
    "x = softmax_layer(x)\n",
    "        \n",
    "# validation code\n",
    "print(input_batch.shape)\n",
    "print(x[0].shape)\n",
    "cnt = 0\n",
    "for p in range(sample_data[\"goal_num\"][0]):\n",
    "    cnt += x[0][p]\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_frame = 20\n",
    "motion_network = MLP(in_channels=(2048+2), out_channels=future_frame*3, hidden_unit=128)\n",
    "\n",
    "\n",
    "#input_batch = torch.cat([feature_batch,gt_goal_batch],dim=2) # (64,500,2050)\n",
    "sample_data[\"target_positions\"][:,-1,:2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cost_map = torch.rand([224,224])\n",
    "print(cost_map.shape)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "def max_margin_loss(batch_negative_index, batch_gt_index, cost_map):\n",
    "    cost = 0\n",
    "    for i in range(len(batch_negative_index)):\n",
    "#        dist = sqrt((batch_negative_index[i][0]-batch_gt_index[i][0]))\n",
    "        cost += cost_map[batch_gt_index[i][0]][batch_gt_index[i][1]] - cost_map[batch_negative_index[i][0]][batch_negative_index[i][1]]\n",
    "    return cost\n",
    "    \n",
    "sampled_index = np.round(sampled_positions_pixels,0).astype(int)\n",
    "target_index = np.round(target_positions_pixels,0).astype(int)\n",
    "\n",
    "#print(target_index)\n",
    "print(max_margin_loss(sampled_index, target_index, cost_map))\n",
    "\n",
    "print(\"time: {} s\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjoW9U1au-eN"
   },
   "source": [
    "## What if I want to change the rasterizer?\n",
    "\n",
    "We can do so easily by building a new rasterizer and new dataset for it. In this example, we change the value to `py_satellite` which renders boxes on an aerial image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PVnOk9Wu-eN"
   },
   "outputs": [],
   "source": [
    "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "data = dataset[50]\n",
    "\n",
    "im = data[\"image\"].transpose(1, 2, 0)\n",
    "im = dataset.rasterizer.to_rgb(im)\n",
    "target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\n",
    "HISTORY_POINTS_COLOR = (0, 128, 25)\n",
    "target_positions_pixels = transform_points(data[\"history_positions\"], data[\"raster_from_agent\"])\n",
    "draw_trajectory(im, target_positions_pixels, HISTORY_POINTS_COLOR, yaws=data[\"history_yaws\"])\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05uDhF5su-eO"
   },
   "source": [
    "## What if I want to visualise an agent?\n",
    "\n",
    "Glad you asked! We can just replace the `EgoDataset` with an `AgentDataset`. Now we're iterating over agents and not the AV anymore, and the first one happens to be the pace car (you will see this one around a lot in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofI0bBXiu-eO"
   },
   "outputs": [],
   "source": [
    "dataset = AgentDataset(cfg, zarr_dataset, rast)\n",
    "data = dataset[0]\n",
    "\n",
    "im = data[\"image\"].transpose(1, 2, 0)\n",
    "im = dataset.rasterizer.to_rgb(im)\n",
    "target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K27NrgJKu-eO"
   },
   "source": [
    "## System Origin and Orientation\n",
    "\n",
    "~At this point you may have noticed that we vertically flip the image before plotting it.~\n",
    "\n",
    "Vertical flipping is not required anymore as it's already performed inside the rasteriser.\n",
    "\n",
    "\n",
    "Further, all our rotations are counter-clockwise for positive value of the angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVWp27SUu-eO"
   },
   "source": [
    "## How does an entire scene look like?\n",
    "\n",
    "It's easy to visualise an individual scene using our toolkit. Both `EgoDataset` and `AgentDataset` provide 2 methods for getting interesting indices:\n",
    "- `get_frame_indices` returns the indices for a given frame. For the `EgoDataset` this matches a single observation, while more than one index could be available for the `AgentDataset`, as that given frame may contain more than one valid agent\n",
    "- `get_scene_indices` returns indices for a given scene. For both datasets, these might return more than one index\n",
    "\n",
    "In this example, we visualise a scene from the ego's point of view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKM_DmM0u-eO"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "cfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\n",
    "rast = build_rasterizer(cfg, dm)\n",
    "dataset = EgoDataset(cfg, zarr_dataset, rast)\n",
    "scene_idx = 1\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, yaws=data[\"target_yaws\"])\n",
    "    clear_output(wait=True)\n",
    "    display(PIL.Image.fromarray(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42dZn5re-HMF"
   },
   "source": [
    "# Introducing a new visualizer\n",
    "starting from l5kit `v1.3.0` you can now use an interactive visualiser (based on Bokeh) to inspect the scene.\n",
    "\n",
    "The visualization can be built starting from individual scenes and allows for a closer inspection over ego, agents and trajectories.\n",
    "\n",
    "`PRO TIP`: try to hover over one agent to show information about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "mapAPI = MapAPI.from_cfg(dm, cfg)\n",
    "for scene_idx in range(10):\n",
    "    out = zarr_to_visualizer_scene(zarr_dataset.get_scene_dataset(scene_idx), mapAPI)\n",
    "    out_vis = visualize(scene_idx, out)\n",
    "    show(out_vis)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of visualise_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
